{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../pyfos/')\n",
    "from pyfos.models.feedforward import build_feedforward\n",
    "from pyfos.models.var_feedforward import build_var_feedforward\n",
    "from pyfos.generic.optimizer import gd_optimizer\n",
    "from pyfos.generic.regularizers import gaus_prior\n",
    "from functools import partial \n",
    "from pyfos.tc.simple import  simple_tc\n",
    "from pyfos.tc.cv import  cv_tc\n",
    "from pyfos.hyperoptimizers.random_search import random_optimize\n",
    "from pyfos.hyperoptimizers.greed_optimize import greed_optimize\n",
    "from pyfos.hyperoptimizers.drmad_optimize import drmad_optimize\n",
    "from pyfos.hyperoptimizers.hoag_optimize import hoag_optimize\n",
    "\n",
    "import theano\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.   1.]\n",
      "[ 12.  12.  12. ...,   1.   1.   1.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = np.load('../../../data/mnist_train_x.npy'), np.load('../../../data/mnist_test_x.npy')\n",
    "Y_train, Y_test = np.load('../../../data/mnist_train_y.npy'), np.load('../../../data/mnist_test_y.npy')\n",
    "hid = 300\n",
    "param_num = X_train.shape[1] * hid + hid + hid*10 + 10\n",
    "np.set_printoptions(threshold=5)\n",
    "lr = theano.shared(10**(-5))\n",
    "\n",
    "init2 = [np.sqrt(2.0/X_train.shape[1]), np.sqrt(2.0/hid)]\n",
    "alphas = theano.shared(np.array([1.0, 1.0]))\n",
    "real_alphas = T.concatenate([T.repeat(alphas[0],  X_train.shape[1] * hid + hid)   , T.repeat(alphas[1],  hid*10 + 10) ])\n",
    "optimizer = partial(gd_optimizer, learning_rate=lr)\n",
    "model_build = partial(build_feedforward,  structure = [X_train.shape[1],hid, 10], \n",
    "                      init_sigmas=init2, use_softmax=True, nonlinearity=lambda x:T.nnet.relu(x), \n",
    "                      log_alphas =real_alphas, bias=True)\n",
    "\n",
    "init2 = [np.sqrt(2.0/X_train.shape[1]), np.sqrt(2.0/hid)]\n",
    "\n",
    "var_alphas = theano.shared(np.array([1.0]*param_num))\n",
    "var_model_build = partial(build_var_feedforward,   param_pool_size=100, structure = [X_train.shape[1],hid, 10],\n",
    "                          use_softmax=True,\n",
    "                          init_sigmas=init2, nonlinearity=lambda x:T.nnet.relu(x), log_alphas =var_alphas, bias=True)\n",
    "\n",
    "#for test\n",
    "alphas.set_value(np.array([12., 1.0]))\n",
    "print alphas.eval()\n",
    "print real_alphas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(abs(np.random.randn(29000)*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07, 1.0000000000000001e-07, 7.500000000000001e-08, 5.0000000000000004e-08, 2.5000000000000002e-08, 1e-08, 7.500000000000001e-09, 5e-09, 2.5e-09]\n",
      "calbirate: alpha\n",
      "[array([-6., -6.])]\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "60000\n",
      "45000\n",
      "60000\n",
      "45000\n",
      "60000\n",
      "45000\n",
      "60000\n",
      "iteration 0, internal loss=-3241450.42473, time=0.00489711761475\n",
      "iteration 500, internal loss=-63511.9208008, time=2.50374889374\n",
      "iteration 1000, internal loss=-63385.1861684, time=2.46901082993\n",
      "iteration 1500, internal loss=-63409.252271, time=2.60503911972\n",
      "iteration 2000, internal loss=-63438.6560387, time=2.73696088791\n",
      "iteration 2500, internal loss=-63415.3516237, time=6.88984584808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-29851077063c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             score = greed_optimize(partial(cv_tc, k =4,  batch_size=100), model_build, optimizer,10,25, 12500,  X_train, Y_train,  [alphas] , \n\u001b[1;32m---> 27\u001b[1;33m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyp_lr_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             ).history[-1][1]\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m              \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m15\u001b[0m \u001b[1;32mor\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/hyperoptimizers/greed_optimize.pyc\u001b[0m in \u001b[0;36mgreed_optimize\u001b[1;34m(trainig_criterion, model_constructor, param_optimizer, trial_num, batch_size, train_iteration_num, X_data, Y_data, hyperparams, limits, lr, verbose)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iteration_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_procedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/tc/cv.pyc\u001b[0m in \u001b[0;36mdo_train\u001b[1;34m(callback)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mcall_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcall_res\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcall_res\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Greed\n",
    "cnter = 1\n",
    "ranges = []\n",
    "for k in xrange(-10, -6):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0,-6.0])], \n",
    "            \n",
    "             [np.array([6.0, 6.0])]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = greed_optimize(partial(cv_tc, k =4,  batch_size=100), model_build, optimizer,10,25, 12500,  X_train, Y_train,  [alphas] , \n",
    "                   lr=hyp_lr_range, verbose=500\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(alphas.eval())) or \\\n",
    "             max(abs(alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.499999999999999e-06]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_lr_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.007500000000000001, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07, 1.0000000000000001e-07, 7.500000000000001e-08, 5.0000000000000004e-08, 2.5000000000000002e-08]\n",
      "[9.999999999999999e-06]\n",
      "calbirate: alpha\n",
      "[array([-6., -6.])]\n"
     ]
    }
   ],
   "source": [
    "#DrMad\n",
    "cnter = 12\n",
    "ranges = []\n",
    "for k in xrange(-9, -3):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "print hyp_lr_range\n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0,-6.0])], \n",
    "             [np.array([6.0, 6.0])]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = drmad_optimize(partial(cv_tc, k =4,  batch_size=25), model_build, optimizer,10,25, 5000,  X_train, Y_train,\n",
    "                                   [alphas] , lr, \n",
    "                   lr=hyp_lr_range, verbose=1000 ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(alphas.eval())) or \\\n",
    "             max(abs(alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.999999999999999e-06]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_lr_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07]\n",
      "calbirate: alpha\n",
      "[array([-6., -6.])]\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "BAD\n",
      "new range [7.5e-05]\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "updating learning rate 1e-21\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "updating learning rate 1e-22\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "updating learning rate 1e-23\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "updating learning rate 1e-24\n",
      "BAD\n",
      "new range [4.9999999999999996e-05]\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "updating learning rate 1e-13\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "updating learning rate 1e-14\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "updating learning rate 1e-15\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "updating learning rate 1e-16\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "updating learning rate 1e-17\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "updating learning rate 1e-18\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "updating learning rate 1e-19\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "updating learning rate 1e-20\n",
      "BAD\n",
      "new range [2.4999999999999998e-05]\n",
      "bad internal learning rate nan None nan\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "updating learning rate 1e-11\n",
      "bad internal learning rate nan nan nan\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "updating learning rate 1e-12\n",
      "BAD\n",
      "new range [9.999999999999999e-06]\n",
      "[array([ 6.,  6.])]\n",
      "final [9.999999999999999e-06]\n"
     ]
    }
   ],
   "source": [
    "#HOAG\n",
    "cnter = 4\n",
    "ranges = []\n",
    "for k in xrange(-8, -4):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0,-6.0])],\n",
    "             [np.array([6.0, 6.0])]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = hoag_optimize(partial(cv_tc, k =4,  batch_size=25), model_build, optimizer,10,25, 500, 5000,  X_train, Y_train,\n",
    "                                   [alphas] , \n",
    "                   lr=hyp_lr_range, verbose=-1,  internal_optimize_learning_rate=10**(-10),\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(alphas.eval())) or \\\n",
    "             max(abs(alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.007500000000000001, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07, 1.0000000000000001e-07, 7.500000000000001e-08, 5.0000000000000004e-08, 2.5000000000000002e-08]\n",
      "[1e-06]\n",
      "calbirate: alpha\n",
      "[array([-6., -6., -6., ..., -6., -6., -6.])]\n",
      "trial 0\n",
      "iteration 0, internal loss=-10630388.6711 time=0.00513911247253\n",
      "iteration 1000, internal loss=-8909577.21513 time=5.16849303246\n",
      "iteration 2000, internal loss=-8909007.07235 time=5.17952299118\n",
      "iteration 3000, internal loss=-8909277.80801 time=5.16933202744\n",
      "iteration 4000, internal loss=-8907367.82361 time=5.17241597176\n",
      "validation score:  -8908367.96809\n",
      "reverse-iteration 0, gradients:[[ 2979.95801286  2979.98363824  2979.95800431 ...,  3034.94478956\n",
      "   3030.66821901  3037.95489358]] time:0.00199699401855\n",
      "max abs grad 3101.80738358\n",
      "reverse-iteration 1000, gradients:[[ 2973.73329558  3042.09899961  2974.883668   ...,  6313.50536022\n",
      "   2071.02649292  3034.4223812 ]] time:1.58714723587\n",
      "max abs grad 9258.48598972\n",
      "reverse-iteration 2000, gradients:[[  2955.04665062   3242.72518864   2959.6838399  ...,  19444.60216899\n",
      "   -5048.67828574   3031.67495881]] time:1.577450037\n",
      "max abs grad 28550.4146481\n",
      "reverse-iteration 3000, gradients:[[  2923.89807797   3581.86220531   2934.35851999 ...,  42428.23521588\n",
      "  -18328.44611697   3029.71262639]] time:1.5758471489\n",
      "max abs grad 65352.6683545\n",
      "reverse-iteration 4000, gradients:[[  2880.28757763   4059.51004963   2898.90770828 ...,  75264.40450088\n",
      "  -37768.27700077   3028.53538396]] time:1.58306503296\n",
      "max abs grad 122769.864175\n",
      "6.0 198653.304571\n",
      "new hyperparam values: [array([-5.99717572, -5.99532502, -5.99714662, ..., -5.8820945 ,\n",
      "       -6.06333949, -5.99697186])]\n",
      "max abs 6.06333949409\n",
      "trial 1\n",
      "iteration 0, internal loss=-10603083.147 time=0.00522708892822\n",
      "iteration 1000, internal loss=-8854617.12635 time=5.16993093491\n",
      "iteration 2000, internal loss=-8853244.55115 time=5.17079591751\n",
      "iteration 3000, internal loss=-8853315.62446 time=5.16956591606\n",
      "iteration 4000, internal loss=-8854474.94056 time=5.17166495323\n",
      "validation score:  -8853322.6946\n",
      "reverse-iteration 0, gradients:[[ 2963.16735411  2952.24220385  2962.99500363 ...,  2394.59134457\n",
      "   3424.45001071  3015.12419147]] time:0.00248003005981\n",
      "max abs grad 3432.01890127\n",
      "reverse-iteration 1000, gradients:[[ 2952.67955835  3011.59933155  2977.47501821 ..., -2080.43069091\n",
      "   2011.28477523  3298.08062546]] time:1.57612514496\n",
      "max abs grad 20479.4325001\n",
      "reverse-iteration 2000, gradients:[[  2921.22702137   3203.45207978   3021.00852511 ..., -19567.12641564\n",
      "   -6316.20534314   3518.15086454]] time:1.5771920681\n",
      "max abs grad 85918.111669\n",
      "reverse-iteration 3000, gradients:[[  2868.80974315   3527.80044854   3093.59552433 ..., -50065.49582962\n",
      "  -21558.02034438   3675.33490871]] time:1.57706809044\n",
      "max abs grad 199466.681331\n",
      "reverse-iteration 4000, gradients:[[  2795.42772371   3984.64443784   3195.23601586 ..., -93575.53893284\n",
      "  -43714.16022852   3769.63275797]] time:1.57421278954\n",
      "max abs grad 361125.141487\n",
      "6.06333949409 570659.692894\n",
      "new hyperparam values: [array([-5.99447454, -5.99075169, -5.99382083, ..., -6.03212873,\n",
      "       -6.13609159, -5.99317081])]\n",
      "max abs 6.25055508622\n",
      "trial 2\n",
      "iteration 0, internal loss=-10541791.933 time=0.00514006614685\n",
      "iteration 1000, internal loss=-8802057.71291 time=5.18039798737\n",
      "iteration 2000, internal loss=-8800164.91611 time=5.16798615456\n",
      "iteration 3000, internal loss=-8801984.27218 time=5.18184304237\n",
      "iteration 4000, internal loss=-8800630.92723 time=5.16863107681\n",
      "validation score:  -8801165.25591\n",
      "reverse-iteration 0, gradients:[[ 2947.19859172  2925.33254725  2943.34501809 ...,  3198.90229872\n",
      "   3955.08532319  2990.87416479]] time:0.00214815139771\n",
      "max abs grad 4959.19349243\n",
      "reverse-iteration 1000, gradients:[[  2961.60524371   2938.05478346   2960.84709401 ..., -11042.51845421\n",
      "    3851.1017512    3370.12540291]] time:1.5739068985\n",
      "max abs grad 12971.3227198\n",
      "reverse-iteration 2000, gradients:[[  3005.51384229   2977.39918232   3013.36329041 ..., -60203.10922955\n",
      "    3264.87104416   3665.08922284]] time:1.57623791695\n",
      "max abs grad 60203.1092296\n",
      "reverse-iteration 3000, gradients:[[   3078.92438744    3043.3657438     3100.89360728 ..., -144282.87002729\n",
      "     2196.39320208    3875.7656246 ]] time:1.57700896263\n",
      "max abs grad 144282.870027\n",
      "reverse-iteration 4000, gradients:[[   3181.83687917    3135.95446792    3223.43804463 ..., -263281.80084744\n",
      "      645.66822496    4002.15460816]] time:1.586165905\n",
      "max abs grad 263281.800847\n",
      "6.25055508622 417028.541464\n",
      "new hyperparam values: [array([-5.99116043, -5.98749666, -5.99044001, ..., -6.44915728,\n",
      "       -6.13747662, -5.98912656])]\n",
      "max abs 6.44915727616\n",
      "trial 3\n",
      "iteration 0, internal loss=-10367512.674 time=0.00520992279053\n",
      "iteration 1000, internal loss=-8753489.15994 time=5.22398400307\n",
      "iteration 2000, internal loss=-8751251.50519 time=5.21337890625\n",
      "iteration 3000, internal loss=-8750120.41478 time=5.21260881424\n",
      "iteration 4000, internal loss=-8751362.13039 time=5.21321082115\n",
      "validation score:  -8751020.29663\n",
      "reverse-iteration 0, gradients:[[ 2927.72618143  2906.37442155  2923.50377653 ...,  7339.85125113\n",
      "   3970.43173059  2969.70064219]] time:0.00227093696594\n",
      "max abs grad 7339.85125113\n",
      "reverse-iteration 1000, gradients:[[  2946.18778373   2983.0102267    2961.25124503 ...,  32114.45978404\n",
      "    5400.51829504   2226.86715495]] time:1.58762907982\n",
      "max abs grad 32114.459784\n",
      "reverse-iteration 2000, gradients:[[   3003.27596719    3234.7742722     3074.83274734 ...,  114866.30770051\n",
      "    14156.44384964    1649.1261196 ]] time:1.58254098892\n",
      "max abs grad 114866.307701\n",
      "reverse-iteration 3000, gradients:[[   3098.99073179    3661.66655804    3264.24828346 ...,  255595.39500054\n",
      "    30238.20839437    1236.47753612]] time:1.582280159\n",
      "max abs grad 255595.395001\n",
      "reverse-iteration 4000, gradients:[[   3233.33207753    4263.68708423    3529.49785339 ...,  454301.72168414\n",
      "    53645.81192925     988.92140453]] time:1.58268404007\n",
      "max abs grad 454301.721684\n",
      "6.44915727616 710699.644554\n",
      "new hyperparam values: [array([-5.98775432, -5.98245668, -5.98656981, ..., -5.73845763,\n",
      "       -6.05313176, -5.9882201 ])]\n",
      "max abs 6.11596555409\n",
      "trial 4\n",
      "iteration 0, internal loss=-10575914.5924 time=0.00526094436646\n",
      "iteration 1000, internal loss=-8697357.28801 time=5.20720911026\n",
      "iteration 2000, internal loss=-8697049.98347 time=5.20569109917\n",
      "iteration 3000, internal loss=-8695917.25683 time=5.21653914452\n",
      "iteration 4000, internal loss=-8695584.92458 time=5.20856499672\n",
      "validation score:  -8695917.78221\n",
      "reverse-iteration 0, gradients:[[ 2907.83722175  2877.19797158  2900.95414309 ...,  1829.71637854\n",
      "   3341.90873012  2958.78188889]] time:0.00222015380859\n",
      "max abs grad 3816.63729907\n",
      "reverse-iteration 1000, gradients:[[ 2899.91520653  2925.50864488  2895.68706694 ...,  2725.95425227\n",
      "     50.43606119  2781.41576774]] time:1.57348299026\n",
      "max abs grad 11727.1135961\n",
      "reverse-iteration 2000, gradients:[[  2876.07584693   3079.69878626   2879.91588531 ...,   6933.79793927\n",
      "  -15807.91121982   2643.46872008]] time:1.5705871582\n",
      "max abs grad 37468.5355753\n",
      "reverse-iteration 3000, gradients:[[  2836.31914294   3339.76839571   2853.64059818 ...,  14453.24743951\n",
      "  -44233.13311293   2544.9407459 ]] time:1.57131290436\n",
      "max abs grad 74532.8357687\n",
      "reverse-iteration 4000, gradients:[[  2780.64509456   3705.71747323   2816.86120558 ...,  25284.30275301\n",
      "  -85225.22961814   2485.83184521]] time:1.5721809864\n",
      "max abs grad 122920.014176\n",
      "6.11596555409 182564.704964\n",
      "new hyperparam values: [array([-5.98504519, -5.97827966, -5.98380018, ..., -5.69904646,\n",
      "       -6.19185613, -5.98575396])]\n",
      "max abs 6.19185612781\n",
      "trial 5\n",
      "iteration 0, internal loss=-10275563.2567 time=0.00526094436646\n",
      "iteration 1000, internal loss=-8645849.93469 time=5.21552586555\n",
      "iteration 2000, internal loss=-8645639.93461 time=5.23315095901\n",
      "iteration 3000, internal loss=-8646469.24294 time=5.20494103432\n",
      "iteration 4000, internal loss=-8647501.6793 time=5.21662902832\n",
      "validation score:  -8646169.01398\n",
      "reverse-iteration 0, gradients:[[ 2892.12148196  2853.23766927  2884.92394035 ...,  1693.38505029\n",
      "   4420.90742491  2948.54129237]] time:0.00217199325562\n",
      "max abs grad 4420.90742491\n",
      "reverse-iteration 1000, gradients:[[ 2907.19250834  2792.59784028  2883.73603965 ...,  2835.28998099\n",
      "   9065.27616543  1033.97773806]] time:1.58171415329\n",
      "max abs grad 42542.8946053\n",
      "reverse-iteration 2000, gradients:[[  2953.33801593   2615.4170235    2880.18037379 ...,   8275.22874933\n",
      "   37110.59467678   -455.0799702 ]] time:1.58281707764\n",
      "max abs grad 128260.922347\n",
      "reverse-iteration 3000, gradients:[[  3030.55800476   2321.69521892   2874.25694277 ...,  18013.20135531\n",
      "   88556.86295896  -1518.63183241]] time:1.58459091187\n",
      "max abs grad 253328.259539\n",
      "reverse-iteration 4000, gradients:[[   3138.85247481    1911.43242656    2865.96574659 ...,   32049.20779893\n",
      "   163404.08101198   -2156.67784857]] time:1.58497810364\n",
      "max abs grad 417744.906182\n",
      "6.19185612781 621287.441339\n",
      "new hyperparam values: [array([-5.98176713, -5.97689445, -5.98094486, ..., -5.6486837 ,\n",
      "       -5.93031382, -5.98812317])]\n",
      "max abs 6.73008345749\n",
      "trial 6\n",
      "iteration 0, internal loss=-10227676.4118 time=0.00518703460693\n",
      "iteration 1000, internal loss=-8600895.11399 time=5.21159601212\n",
      "iteration 2000, internal loss=-8601421.11361 time=5.2141919136\n",
      "iteration 3000, internal loss=-8602061.09766 time=5.20299887657\n",
      "iteration 4000, internal loss=-8601048.51591 time=5.20125198364\n",
      "validation score:  -8601129.69977\n",
      "reverse-iteration 0, gradients:[[ 2873.21355787  2845.39566612  2868.49219927 ...,  1545.53410478\n",
      "   2625.90784478  2958.13825712]] time:0.00220489501953\n",
      "max abs grad 12845.5861337\n",
      "reverse-iteration 1000, gradients:[[ 2865.11851323  2945.68462388  2925.59589912 ...,  2437.42725361\n",
      "   1138.95303979  2121.2946869 ]] time:1.56583094597\n",
      "max abs grad 73985.2935998\n",
      "reverse-iteration 2000, gradients:[[ 2840.69963139  3284.73200373  3098.38737777 ...,  7066.55166492\n",
      "  -7784.19689808  1470.43701961]] time:1.56529903412\n",
      "max abs grad 246628.586887\n",
      "reverse-iteration 3000, gradients:[[  2799.95691234   3862.53780569   3386.86663521 ...,  15432.9073387\n",
      "  -24143.54196884   1005.56525525]] time:1.5637409687\n",
      "max abs grad 530775.465995\n",
      "reverse-iteration 4000, gradients:[[  2742.89035609   4679.10202975   3791.03367145 ...,  27536.49427496\n",
      "  -47939.08217247    726.67939381]] time:1.57413721085\n",
      "max abs grad 926425.930925\n",
      "6.73008345749 1433017.13158\n",
      "new hyperparam values: [array([-5.97909754, -5.9711612 , -5.97663455, ..., -5.60532409,\n",
      "       -6.00944969, -5.9874894 ])]\n",
      "max abs 6.69850302036\n",
      "trial 7\n",
      "iteration 0, internal loss=-10172242.4215 time=0.00515604019165\n",
      "iteration 1000, internal loss=-8550031.06611 time=5.20257091522\n",
      "iteration 2000, internal loss=-8549129.04225 time=5.17444396019\n",
      "iteration 3000, internal loss=-8551455.10751 time=5.16049218178\n",
      "iteration 4000, internal loss=-8549098.04142 time=5.17177510262\n",
      "validation score:  -8549413.5926\n",
      "reverse-iteration 0, gradients:[[ 2857.9119093   2812.88911757  2843.85992372 ...,  1425.66429197\n",
      "   3066.00498968  2945.05402593]] time:0.00218820571899\n",
      "max abs grad 12055.4632629\n",
      "reverse-iteration 1000, gradients:[[ 2877.44690069  2808.3412012   2813.49361643 ...,  2206.91849425\n",
      "   1260.90175568  3412.63780855]] time:1.57867598534\n",
      "max abs grad 119117.082678\n",
      "reverse-iteration 2000, gradients:[[ 2937.52018223  2794.52081279  2722.41030003 ...,  6495.96973671\n",
      "  -8056.67196681  3776.30253736]] time:1.57725095749\n",
      "max abs grad 432910.935667\n",
      "reverse-iteration 3000, gradients:[[  3038.13175392   2771.42795235   2570.60997452 ...,  14292.81801932\n",
      "  -24886.7161778    4036.04821236]] time:1.57801604271\n",
      "max abs grad 953437.022229\n",
      "reverse-iteration 4000, gradients:[[  3179.28161576   2739.06261986   2358.09263989 ...,  25597.46334211\n",
      "  -49229.23087727   4191.87483355]] time:1.58027505875\n",
      "max abs grad 1680695.34236\n",
      "6.69850302036 2613648.64277\n",
      "new hyperparam values: [array([-5.97573678, -5.96846373, -5.97454939, ..., -5.56493075,\n",
      "       -6.0904983 , -5.98324561])]\n",
      "max abs 6.36127045556\n",
      "trial 8\n",
      "iteration 0, internal loss=-10166192.8162 time=0.00516200065613\n",
      "iteration 1000, internal loss=-8495595.71561 time=5.17360091209\n",
      "iteration 2000, internal loss=-8494403.76705 time=5.1832048893\n",
      "iteration 3000, internal loss=-8494486.03894 time=5.1707880497\n",
      "iteration 4000, internal loss=-8494078.94331 time=5.1709830761\n",
      "validation score:  -8495043.94783\n",
      "reverse-iteration 0, gradients:[[ 2838.75644958  2797.7823424   2832.02414523 ...,  1295.18214011\n",
      "   3595.49703194  2912.71161006]] time:0.00221180915833\n",
      "max abs grad 6170.27754971\n",
      "reverse-iteration 1000, gradients:[[ 2828.93406419  2880.35046686  2932.27717297 ...,  2383.59053102\n",
      "   3658.22927979  1948.19084173]] time:1.58040714264\n",
      "max abs grad 11820.6559131\n",
      "reverse-iteration 2000, gradients:[[ 2799.60208427  3151.93022729  3230.28722879 ...,  7591.64912331\n",
      "   3950.77177099  1198.0318399 ]] time:1.58122396469\n",
      "max abs grad 48018.8528845\n",
      "reverse-iteration 3000, gradients:[[  2750.76050981   3612.52162369   3726.05431267 ...,  16919.35791698\n",
      "    4473.12450554    662.23460455]] time:1.5901761055\n",
      "max abs grad 102424.313365\n",
      "reverse-iteration 4000, gradients:[[  2682.4093408    4262.12465607   4419.57842461 ...,  30366.71691202\n",
      "    5225.28748343    340.7991357 ]] time:1.58381819725\n",
      "max abs grad 175037.037353\n",
      "6.36127045556 265757.110335\n",
      "new hyperparam values: [array([-5.97314213, -5.96336392, -5.96923952, ..., -5.51701665,\n",
      "       -6.08429213, -5.98301189])]\n",
      "max abs 6.6270275659\n",
      "trial 9\n",
      "iteration 0, internal loss=-10156862.6062 time=0.00522589683533\n",
      "iteration 1000, internal loss=-8448332.00144 time=5.15896487236\n",
      "iteration 2000, internal loss=-8449839.61732 time=5.15704393387\n",
      "iteration 3000, internal loss=-8449453.07924 time=5.16966891289\n",
      "iteration 4000, internal loss=-8447839.80498 time=5.16761898994\n",
      "validation score:  -8448822.51402\n",
      "reverse-iteration 0, gradients:[[ 2824.05823865  2769.34955903  2802.12073735 ...,  1185.67777483\n",
      "   3555.50501694  2908.60864059]] time:0.00218915939331\n",
      "max abs grad 10461.7346136\n",
      "reverse-iteration 1000, gradients:[[ 2820.53928367  2780.81951186  2770.3203862  ...,  2119.70524149\n",
      "   4433.58083969  2511.59998808]] time:1.57525992393\n",
      "max abs grad 18850.394545\n",
      "reverse-iteration 2000, gradients:[[ 2809.99471601  2815.9844417   2677.77676553 ...,  6909.57460621\n",
      "   8815.93373125  2202.82528435]] time:1.57748317719\n",
      "max abs grad 85090.4591682\n",
      "reverse-iteration 3000, gradients:[[  2792.42453567   2874.84434858   2524.48987534 ...,  15555.28586898\n",
      "   16702.56369163   1982.28452939]] time:1.57679605484\n",
      "max abs grad 188258.459256\n",
      "reverse-iteration 4000, gradients:[[  2767.82874265   2957.39923247   2310.45971562 ...,  28056.83902981\n",
      "   28093.47072084   1849.9777232 ]] time:1.59018802643\n",
      "max abs grad 328354.394808\n",
      "6.6270275659 505182.79645\n",
      "new hyperparam values: [array([-5.97040589, -5.96030039, -5.96720353, ..., -5.4726207 ,\n",
      "       -6.04132012, -5.98120598])]\n",
      "max abs 7.13221036235\n",
      "validation score:  -8410188.636\n",
      "[array([ 6.,  6.,  6., ...,  6.,  6.,  6.])]\n",
      "trial 0\n",
      "iteration 0, internal loss=-74637.0785133 time=0.00535702705383\n",
      "iteration 1000, internal loss=-49155.1240036 time=5.32373285294\n",
      "iteration 2000, internal loss=-47919.6034508 time=5.33671307564\n",
      "iteration 3000, internal loss=-47825.2158488 time=5.32899785042\n",
      "iteration 4000, internal loss=-48083.9864987 time=5.32500910759\n",
      "validation score:  -48239.3616831\n",
      "reverse-iteration 0, gradients:[[-0.99999986 -0.99999984 -0.99999988 ..., -0.99999986 -0.99999984\n",
      "  -0.99998792]] time:0.00218105316162\n",
      "max abs grad 0.999999887465\n",
      "reverse-iteration 1000, gradients:[[-0.99999986 -0.99999984 -0.99999988 ..., -0.99999956 -0.99999973\n",
      "  -0.99998733]] time:1.5481979847\n",
      "max abs grad 0.999999910526\n",
      "reverse-iteration 2000, gradients:[[-0.99999986 -0.99999984 -0.99999988 ..., -0.99999917 -0.99999953\n",
      "  -0.99998686]] time:1.54631304741\n",
      "max abs grad 0.999999991839\n",
      "reverse-iteration 3000, gradients:[[-0.99999986 -0.99999983 -0.99999988 ..., -0.99999869 -0.99999924\n",
      "  -0.99998653]] time:1.55126404762\n",
      "max abs grad 1.00000013127\n",
      "reverse-iteration 4000, gradients:[[-0.99999986 -0.99999983 -0.99999988 ..., -0.99999812 -0.99999887\n",
      "  -0.99998633]] time:1.5539867878\n",
      "max abs grad 1.00000032881\n",
      "6.0 1.00000058419\n",
      "new hyperparam values: [array([ 5.999999,  5.999999,  5.999999, ...,  5.999999,  5.999999,\n",
      "        5.999999])]\n",
      "max abs 5.99999900001\n",
      "trial 1\n",
      "iteration 0, internal loss=-92592.5208425 time=0.00531196594238\n",
      "iteration 1000, internal loss=-48264.2630315 time=5.32760500908\n",
      "iteration 2000, internal loss=-48768.85033 time=5.34010696411\n",
      "iteration 3000, internal loss=-48107.8555219 time=5.33989787102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3726b686b098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m             score = drmad_optimize(partial(simple_tc, batch_size=25), var_model_build, optimizer,10,25, 5000,  X_train, Y_train,\n\u001b[0;32m     26\u001b[0m                                    \u001b[1;33m[\u001b[0m\u001b[0mvar_alphas\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyp_lr_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             ).history[-1][1]\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_alphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m              \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_alphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m15\u001b[0m \u001b[1;32mor\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/hyperoptimizers/drmad_optimize.pyc\u001b[0m in \u001b[0;36mdrmad_optimize\u001b[1;34m(trainig_criterion, model_constructor, param_optimizer, trial_num, batch_size, train_iteration_num, X_data, Y_data, hyperparams, learning_rate_param, lr, lr_for_learning_rate, limits, lr_limits, verbose, use_hessian)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iteration_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mres\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtraining_procedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/tc/simple.pyc\u001b[0m in \u001b[0;36mdo_train\u001b[1;34m(callback)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mcres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DrMad-var\n",
    "cnter = 16\n",
    "ranges = []\n",
    "for k in xrange(-9, -3):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "print hyp_lr_range\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0]*param_num)],\n",
    "             [np.array([6.0]*param_num)]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            var_alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = drmad_optimize(partial(simple_tc, batch_size=25), var_model_build, optimizer,10,25, 5000,  X_train, Y_train,\n",
    "                                   [var_alphas] , lr, \n",
    "                   lr=hyp_lr_range, verbose=1000\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(var_alphas.eval())) or \\\n",
    "             max(abs(var_alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DrMad-var additional correction\n",
    "cnter = 10\n",
    "ranges = []\n",
    "for k in xrange(-9,     -5):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "#for h in [  [np.array([-6.0]*param_num)],\n",
    "#             [np.array([6.0]*param_num)]]:\n",
    "#print h\n",
    "while True:\n",
    "            h = []\n",
    "            for _ in xrange(param_num):\n",
    "                h.append(np.random.uniform(low=-6, high=6))\n",
    "            \n",
    "            h = np.array(h)\n",
    "            print h \n",
    "            var_alphas.set_value(h)\n",
    "\n",
    "\n",
    "\n",
    "            score = drmad_optimize(partial(simple_tc, batch_size=25), var_model_build, optimizer,5,25, 500,  X_train, Y_train,\n",
    "                                   [var_alphas] , lr, \n",
    "                   lr=hyp_lr_range, verbose=2500\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(alphas.eval())) or \\\n",
    "             max(abs(alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original, bests = [], []\n",
    "for _ in xrange(1):\n",
    "    print 'attemp'\n",
    "    \n",
    "    hyp_lr_range = [1.0] \n",
    "   \n",
    "    alphas_value =choicer()\n",
    "    print alphas_value\n",
    "    \n",
    "    alphas.set_value(alphas_value)\n",
    "\n",
    "    bests.append(greed_optimize(partial(cv_tc, k =4,  batch_size=75), model_build, optimizer,5, 10**3,  X_train, Y_train,  [alphas] , \n",
    "               lr=hyp_lr_range, verbose=100, limits=[(-10, 10) ]\n",
    "    ))\n",
    "    \n",
    "    print 'final'\n",
    "    print bests[-1].history[-1][-1]\n",
    "    print lr.eval(), alphas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print param_num\n",
    "h = []\n",
    "for _ in xrange(param_num):\n",
    "    h.append(np.random.uniform(low=-6, high=6))\n",
    "            \n",
    "h = np.array(h)\n",
    "print h \n",
    "var_alphas.set_value(h)\n",
    "\n",
    "score = drmad_optimize(partial(simple_tc, batch_size=25), var_model_build, optimizer,2,25, 2500,  X_train, Y_train,\n",
    "                                   [var_alphas] , lr, \n",
    "                   lr=[2.5*10**(-6)], verbose=500\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.007500000000000001, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07]\n",
      "[0.00075]\n",
      "calbirate: alpha\n",
      "[array([-6., -6., -6., ..., -6., -6., -6.])]\n"
     ]
    }
   ],
   "source": [
    "#VAR_Greed\n",
    "cnter = 0\n",
    "ranges = []\n",
    "for k in xrange(-8, -3):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "cnter=  5\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "print hyp_lr_range\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0]*param_num)],\n",
    "             [np.array([6.0]*param_num)]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            var_alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = greed_optimize(partial(simple_tc,  batch_size=25), var_model_build,\n",
    "                                   optimizer,5,25, 2500,  X_train, Y_train,  [var_alphas] , \n",
    "                   lr=hyp_lr_range, verbose=1000\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(var_alphas.eval())) or \\\n",
    "             max(abs(var_alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.007500000000000001, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07]\n",
      "calbirate: alpha\n",
      "[array([-6., -6., -6., ..., -6., -6., -6.])]\n",
      "trial  0\n",
      "iteration 0, internal loss=-10640407.4063\n",
      "iteration 500, internal loss=-8911445.85713\n",
      "iteration 1000, internal loss=-8908114.73816\n",
      "iteration 1500, internal loss=-8908636.8158\n",
      "iteration 2000, internal loss=-8909694.88247\n",
      "iteration 2500, internal loss=-8906962.02024\n"
     ]
    }
   ],
   "source": [
    "#VAR_hoag\n",
    "cnter = 2\n",
    "ranges = []\n",
    "for k in xrange(-8, -3):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.0]*param_num)],\n",
    "             [np.array([6.0]*param_num)]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            var_alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = hoag_optimize(partial(simple_tc,  batch_size=25), var_model_build, optimizer,10,25,500, 5000,  X_train, Y_train,\n",
    "                                   [var_alphas] , \n",
    "                   lr=hyp_lr_range, verbose=500,  internal_optimize_learning_rate=10**(-10),\n",
    "            ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(var_alphas.eval())) or \\\n",
    "             max(abs(var_alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_lr_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "training_procedure = cv_tc( model_build,  optimizer, X_train, Y_train, k=4, validation_part=0.25,  batch_size=75 )\n",
    "X = T.matrix()\n",
    "result = []\n",
    "import cPickle\n",
    "with open('results_cv_greed.pckl','rb') as inp:\n",
    "    bests = cPickle.load(inp)\n",
    "for b in bests:\n",
    "    for i in xrange(len(b.history)):\n",
    "        training_procedure.models[0].respawn()\n",
    "        \n",
    "        alphas.set_value(b.history[i][0][0])\n",
    "        lr.set_value(0.01)\n",
    "        \n",
    "    \n",
    "        for _ in xrange(100):\n",
    "            training_procedure.do_train()\n",
    "        print i, training_procedure.do_validation()\n",
    "        b.history[i] = [b.history[i][0], training_procedure.do_validation()]\n",
    "    \"\"\"\n",
    "    bests.append( random_optimize(partial(cv_tc, k =4,  batch_size=75), model_build, optimizer, 50, 10, X_train, Y_train,  [alphas, lr] ,\n",
    "     [alpha_ranges, lr_ranges], verbose=100))\n",
    "    X = T.matrix()    \n",
    "    model = model_build(dataset_size=100)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "#print np.mean(result)\n",
    "#print np.std(result)\n",
    "import cPickle\n",
    "with open('results_cv_greed.pckl','wb') as out:\n",
    "    cPickle.dump(bests, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_values = []\n",
    "for b in bests:\n",
    "    history = []\n",
    "    for i in xrange(0, len(b.history)):\n",
    "        \n",
    "        best_value = max([h[1] for h in b.history[:i+1]])\n",
    "        \n",
    "        history.append(best_value)\n",
    "        if np.isnan(history[-1]):\n",
    "            continue\n",
    "        best_values.append(history[-1])\n",
    "print np.mean(best_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(matrix)\n",
    "for i in bests:\n",
    "    i = i.best_values[0]\n",
    "    i = np.log10(np.exp(i)**2)\n",
    "    if i[0]>10 or i[1] >10:\n",
    "        continue\n",
    "    \n",
    "    plt.scatter(i[0]+np.random.randn(1)*0.1,i[1]+np.random.randn(1)*0.1, c='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in bests:\n",
    "    print np.array(i.best_values[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.007500000000000001, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 9.999999999999999e-05, 7.5e-05, 4.9999999999999996e-05, 2.4999999999999998e-05, 9.999999999999999e-06, 7.499999999999999e-06, 4.9999999999999996e-06, 2.4999999999999998e-06, 1e-06, 7.5e-07, 5e-07, 2.5e-07, 1.0000000000000001e-07, 7.500000000000001e-08, 5.0000000000000004e-08, 2.5000000000000002e-08]\n",
      "[9.999999999999999e-06]\n",
      "calbirate: alpha\n",
      "[array([-6.59,  5.63])]\n",
      "trial 0\n",
      "iteration 0, internal loss=-6336274.69145 time=0.00168085098267\n",
      "iteration 1000, internal loss=-1053007.17248 time=1.42700695992\n",
      "iteration 2000, internal loss=-1052848.4511 time=1.42972898483\n",
      "iteration 3000, internal loss=-1052997.01786 time=1.42304611206\n",
      "iteration 4000, internal loss=-1052867.02597 time=1.43135690689\n",
      "validation score:  -1052950.19521\n",
      "reverse-iteration 0, gradients:[[-5944.02432941   -10.99993813]] time:0.0141620635986\n",
      "max abs grad 5944.02432941\n",
      "reverse-iteration 1000, gradients:[[ -1.35819891e+04  -1.09999377e+01]] time:3.00770401955\n",
      "max abs grad 13581.989064\n",
      "reverse-iteration 2000, gradients:[[ -1.76746250e+04  -1.09999373e+01]] time:3.00365495682\n",
      "max abs grad 17674.6250002\n",
      "reverse-iteration 3000, gradients:[[ -1.82219321e+04  -1.09999371e+01]] time:2.99757099152\n",
      "max abs grad 18221.9321382\n",
      "reverse-iteration 4000, gradients:[[ -1.52239105e+04  -1.09999369e+01]] time:2.99553513527\n",
      "max abs grad 15223.9104779\n",
      "6.59 8688.87426146\n",
      "new hyperparam values: [array([-6.67688874,  5.62989   ])]\n",
      "max abs 6.67688874261\n",
      "trial 1\n",
      "iteration 0, internal loss=-7395300.51336 time=0.00158405303955\n",
      "iteration 1000, internal loss=-1052575.18907 time=1.4253680706\n",
      "iteration 2000, internal loss=-1052565.51116 time=1.42340397835\n",
      "iteration 3000, internal loss=-1052450.66259 time=1.43780493736\n",
      "iteration 4000, internal loss=-1052367.0597 time=1.42280602455\n",
      "validation score:  -1052430.61969\n",
      "reverse-iteration 0, gradients:[[-5954.72317608   -10.99992374]] time:0.00322294235229\n",
      "max abs grad 5954.72317608\n",
      "reverse-iteration 1000, gradients:[[ -1.17902865e+04  -1.09999235e+01]] time:3.00498485565\n",
      "max abs grad 11790.2865164\n",
      "reverse-iteration 2000, gradients:[[ -1.11593653e+04  -1.09999233e+01]] time:3.0013718605\n",
      "max abs grad 11159.3652775\n",
      "reverse-iteration 3000, gradients:[[-4061.95945938   -10.99992314]] time:3.00354003906\n",
      "max abs grad 4061.95945938\n",
      "reverse-iteration 4000, gradients:[[ 9501.93093796   -10.99992305]] time:3.00504207611\n",
      "max abs grad 9501.93093796\n",
      "6.67688874261 29509.0455305\n",
      "new hyperparam values: [array([-6.38179829,  5.62978   ])]\n",
      "max abs 6.38179828731\n",
      "trial 2\n",
      "iteration 0, internal loss=-4521555.75144 time=0.00159001350403\n",
      "iteration 1000, internal loss=-1054298.89053 time=1.41417098045\n",
      "iteration 2000, internal loss=-1054267.50841 time=1.41634607315\n",
      "iteration 3000, internal loss=-1054147.7938 time=1.42252492905\n",
      "iteration 4000, internal loss=-1054291.85797 time=1.43002891541\n",
      "validation score:  -1054201.85012\n",
      "reverse-iteration 0, gradients:[[-5914.43799401   -10.9999347 ]] time:0.00325894355774\n",
      "max abs grad 5914.43799401\n",
      "reverse-iteration 1000, gradients:[[ -1.80235127e+04  -1.09999345e+01]] time:3.00577116013\n",
      "max abs grad 18023.5126501\n",
      "reverse-iteration 2000, gradients:[[ -3.15953568e+04  -1.09999343e+01]] time:3.00411605835\n",
      "max abs grad 31595.3567769\n",
      "reverse-iteration 3000, gradients:[[ -4.66299704e+04  -1.09999341e+01]] time:2.99653315544\n",
      "max abs grad 46629.9703746\n",
      "reverse-iteration 4000, gradients:[[ -6.31273534e+04  -1.09999341e+01]] time:2.99884390831\n",
      "max abs grad 63127.3534429\n",
      "6.38179828731 81068.8151762\n",
      "new hyperparam values: [array([-7.19248644,  5.62967   ])]\n",
      "max abs 7.19248643907\n",
      "trial 3\n",
      "iteration 0, internal loss=-18843523.6388 time=0.00160813331604\n",
      "iteration 1000, internal loss=-1049514.62667 time=1.41462016106\n",
      "iteration 2000, internal loss=-1049460.02308 time=1.44020795822\n",
      "iteration 3000, internal loss=-1049449.48778 time=1.43900108337\n",
      "iteration 4000, internal loss=-1049429.06596 time=1.51707482338\n",
      "validation score:  -1049494.26787\n",
      "reverse-iteration 0, gradients:[[-5945.13527561   -10.99995393]] time:0.00330781936646\n",
      "max abs grad 5945.13527561\n",
      "reverse-iteration 1000, gradients:[[ -1.25228984e+05  -1.09999538e+01]] time:2.99945688248\n",
      "max abs grad 125228.983605\n",
      "reverse-iteration 2000, gradients:[[  8.25060142e+04  -1.09999536e+01]] time:2.99152994156\n",
      "max abs grad 82506.0141548\n",
      "reverse-iteration 3000, gradients:[[  6.17259858e+05  -1.09999536e+01]] time:2.98908686638\n",
      "max abs grad 617259.858004\n",
      "reverse-iteration 4000, gradients:[[  1.47903255e+06  -1.09999535e+01]] time:2.99061894417\n",
      "max abs grad 1479032.54794\n",
      "7.19248643907 2666471.94652\n",
      "new hyperparam values: [array([ 19.47223303,   5.62956   ])]\n",
      "max abs 19.4722330262\n",
      "trial 4\n",
      "iteration 0, internal loss=-1213762.46206 time=0.00153112411499\n",
      "iteration 1000, internal loss=-1208710.4125 time=1.42713689804\n",
      "iteration 2000, internal loss=-1208604.40558 time=1.4234290123\n",
      "iteration 3000, internal loss=-1208532.33432 time=1.4261841774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c9caa0b086e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m             score = drmad_optimize(partial(cv_tc, k =4,  batch_size=25), model_build, optimizer,10,25, 5000,  X_train, Y_train,\n\u001b[0;32m     26\u001b[0m                                    \u001b[1;33m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                    lr=hyp_lr_range, verbose=1000 ).history[-1][1]\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m              \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m15\u001b[0m \u001b[1;32mor\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m'BAD'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/hyperoptimizers/drmad_optimize.pyc\u001b[0m in \u001b[0;36mdrmad_optimize\u001b[1;34m(trainig_criterion, model_constructor, param_optimizer, trial_num, batch_size, train_iteration_num, X_data, Y_data, hyperparams, learning_rate_param, lr, lr_for_learning_rate, limits, lr_limits, verbose, use_hessian)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iteration_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mres\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtraining_procedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/legin/svn074/Bakhteev2017Hypergrad/code/pyfos/tc/cv.pyc\u001b[0m in \u001b[0;36mdo_train\u001b[1;34m(callback)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0msub_indices\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0msetsize\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0m_ceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# table size for big sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0msetsize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"keys\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;31m# An n-length list is smaller than a k-length set, or this is a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# mapping type so the other algorithm wouldn't work.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DrMad\n",
    "cnter = 12\n",
    "ranges = []\n",
    "for k in xrange(-9, -3):\n",
    "    for j in xrange(1,5):\n",
    "        ranges.append(10**(k)*(j*25))\n",
    "ranges.sort(reverse=True)\n",
    "print ranges\n",
    "hyp_lr_range = [ranges[cnter]] \n",
    "print hyp_lr_range\n",
    "\n",
    "print 'calbirate: alpha'\n",
    "found = False\n",
    "for h in [  [np.array([-6.59,5.63])]]:\n",
    "    \n",
    "    print h\n",
    "    while True:\n",
    "\n",
    "            \n",
    "            \n",
    "            alphas.set_value(h[0])\n",
    "\n",
    "\n",
    "\n",
    "            score = drmad_optimize(partial(cv_tc, k =4,  batch_size=25), model_build, optimizer,10,25, 5000,  X_train, Y_train,\n",
    "                                   [alphas] , lr, \n",
    "                   lr=hyp_lr_range, verbose=1000 ).history[-1][1]\n",
    "            if lr.eval()>1 or np.isnan(lr.eval()) or np.isnan(np.sum(alphas.eval())) or \\\n",
    "             max(abs(alphas.eval()))>15 or \\\n",
    "            np.isnan(score) or np.isinf(score):\n",
    "                print 'BAD'\n",
    "                cnter+=1\n",
    "                hyp_lr_range = [ranges[cnter]] \n",
    "                print 'new range', hyp_lr_range\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "            \n",
    "print 'final', hyp_lr_range            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
