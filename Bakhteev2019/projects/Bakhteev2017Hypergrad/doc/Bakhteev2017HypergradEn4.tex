\documentclass[smallcondensed]{svjour3} 
\smartqed
\journalname{Annals of Operations Research}
\usepackage{cmap}

%\usepackage[utf8]{inputenc}
%\usepackage[T2A]{fontenc}

\usepackage{amssymb,amsfonts}%,amsthm,amsmath}
\usepackage{mathtext,enumerate,float}
%\usepackage[russian, english]{babel}
%\usepackage{graphicx}
\usepackage{tabularx}

%\usepackage{enumerate}
%\usepackage{fancyhdr}
%\usepackage{a4wide}
%\usepackage{cite}

\usepackage{caption}
\usepackage[caption=false]{subfig}
%\usepackage{multicol}

\usepackage{mathtools}
\sloppy
%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

%\begin{frontmatter}


%\title{Comprehensive analysis of gradient-based hyperparameter optimization algorithms\footnote}
%\date{}
%\maketitle

\title{Comprehensive analysis of gradient-based hyperparameter optimization algorithms\thanks{The research was made possible by Government of the Russian Federation (Agreement No. 05.Y09.21.0018)  and FASIE project No. 44116. This paper contains results of the project Statistical methods of machine learning, which is carried out within the framework of the Program "Center of Big Data Storage and Analysis" of the National Technology Initiative Competence Center. It is supported by the Ministry of Science and Higher Education of the Russian Federation according to the agreement between the M.V. Lomonosov Moscow State University and the Foundation of project support of the National Technology Initiative from 11.12.2018, No 13/1251/2018.}}% \tnoteref{label1}}


\author{O.\,Y.~Bakhteev        \and
        V.\,V.~Strijov %etc.
}

\institute{O.\,Y.~Bakhteev \at
              Moscow Institute of Physics and Technology \\
              Tel.: +74991354163\\             
              \email{bakhteev@phystech.edu}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           V.\,V.~Strijov \at
              Moscow Institute of Physics and Technology 
            \and 
            FRCCSC of the Russian Academy of Sciences
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle



%\author[mipt]{O.\,Y.~Bakhteev \corref{cor1}}
%\ead{bakhteev@phystech.edu}

%\author[mipt,vc]{V.\,V.~Strijov}
%\ead{strijov@ccas.ru}
%\cortext[cor1]{Corresponding author}
%\address[mipt]{Moscow Institute of Physics and Technology}
%\address[vc]{FRCCSC of the Russian Academy of Sciences}

%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


\begin{abstract} 
%The paper investigates the problem of hyperparameter optimization problem for models with large number of parameters.
%For this problem solution the paper proposes gradient-based algorithms. For the model selection propose probabilistic interpretation of parameter distribution. The paper compares two model selection criterion: cross-validation and evidence lower bound that is a lower bound of marginal likelihood. The experiment is conducted on a series of datasets.\bigskip

%The paper investigates the problem of hyperparameter optimization problem.
%The paper analyzes gradient-based algorithms of hyperparameter optimization: HOAG, DrMad and greedy algorithm.  The complexity of the parameter optimization is comparable to the hyperparameter optimization complexity, therefore  optimize the model parameters and hyperparameters is a single procedure. The paper compares two model selection criterion: cross-validation and evidence lower bound that is a lower bound of marginal likelihood. 
The paper investigates  hyperparameter optimization problem. Hyperparameters are the parameters of model parameter distribution. The adequate choice of hyperparameter values prevents model overfit and allows it to obtain higher predictive performance.  Neural network models with large amount of hyperparameters are analyzed. The hyperparameter optimization for models is computationally expensive. The paper proposes modifications of various gradient-based methods to simultaneously optimize many hyperparameters. The paper compares the experiment results with the random search. The main impact of the paper is hyperparameter optimization algorithms analysis for the models with high amount of parameters. To select precise and stable models the authors suggest to use two model selection criteria: cross-validation and evidence lower bound. The experiments show that the models optimized using the evidence lower bound give higher error rate
than the models obtained using cross-validation. These models also show greater stability when data is noisy.  The evidence lower bound usage
is preferable when the model tends to overfit or when the cross-validation is computationally expensive. The algorithms are evaluated on  regression and classification datasets.


\keywords{gradient descent \and  hyperparameter optimization \and model selection \and neural networks \and classification \and regression}
%\begin{keyword}  gradient descent, stochastic hyperparameter search, hyperparameter optimization, model selection, neural networks, classification, regression.
%\end{keyword}
%\end{frontmatter}
\end{abstract}
\section{Introduction}
\label{intro}
The paper analyzes hyperparameter optimization problem. \textit{Model} is a function superposition, which solves a classification or regression problem. \textit{Model hyperparameters} are the parameters of model parameter distribution.

The paper focuses on the neural network models. The parameter optimization problem is computationally expensive because of training loss non-convexity. The amount of model parameters can reach millions~\cite{hinton_rbm}  and such models optimization requires multiple days~\cite{suts}.   The hyperparameter values can influence significantly the quality of models~\cite{journal1,journal2}. These two facts make the hyperparameter optimization problem very important. The adequate choice of hyperparameter values also helps to prune redundant parameters to make the model more compact and stable~\cite{nips,ard}. Comprehensive analysis of the hyperparameters and hyperparameter optimization algorithms leads to challenges addressed by operations research.

In this paper gradient-based algorithms are analyzed. They optimize a large amount of hyperparameters within a short time in contrast to gradient-free algorithms~\cite{hyper}.  The complexity of hyperparameter optimization is comparable to the model parameter optimization complexity, therefore, the parameters and hyperparameters are  optimized in a single procedure. As a baseline, a random search algorithm is used. The pros and cons of all the algorithms  
are illustrated in Table~\ref{table:algo_descr}. Opposing to papers~\cite{hyper_mad,hyper_hoag,hyper_greed} this research does not focus on the hold-out cross-validation and analyzes algorithms in general. The analyzed algorithms are implemented as a toolbox~\cite{pyfos}. The authors propose a number of analyzed algorithms modifications: we extend all the algorithms to work with validation loss that can contain hyperparameters, for DrMad algorithm~\cite{hyper_mad} we add an additional parameter that regularizes a length of analyzed parameter update trajectory, for HOAG algorithm~\cite{hyper_hoag} we use stochastic gradient descent for large linear system solving because of high dimension of the linear system.   The main impact of the paper is the analysis of algorithm behavior with various validation loss functions and a quality and stability analysis of the resulting models. The experiments are conducted using cross-validation and evidence lower bound as a model selection criterion. Opposing to the paper~\cite{hyper_hoag}, where  the optimization algorithms comparison can also be found, this paper presents the results on the large datasets, such as  WISDM~\cite{wisdm} and MNIST~\cite{mnist}, where the number of hyperparameters is significant. 
The experimental results show that the gradient-based algorithms are significantly more effective than random search-based algorithms when the number of hyperparameters is large. 

\begin{table}
\footnotesize

\begin{tabularx}{\textwidth}{|p{2cm}|p{2cm}|X|X|}
\hline
\bf Algorthm & \bf Type  & \bf Pros & \bf Cons  \\ \hline
Random search & Stochastic & Easy to implement & Ineffective in high dimensions (curse of dimension)\\ \hline
Greedy~\cite{hyper_greed} & Gradient-based & Can be used in inner model parameter optimization& Greedy and non-optimal: the algorithm is correct whenever the loss function Hessian is identical. \\ \hline
HOAG~\cite{hyper_hoag} & Gradient-based & Fast convergence & The algorithm requires additional parameter configuration: it is required to solve the linear system, which depends on the Hessian of a loss function. The error in linear system solution influences the final model quality.\\ \hline 
DrMAD~\cite{hyper_mad} & Gradient-based & Considers the trajectory of parameter updates and the optimization algorithm.& Can be instable because of gradient explosion or vanishing. The algorithm is correct when the trajectory of parameter update is linear, which is mostly not true for the complex non-convex models. \\ \hline
\end{tabularx}

\caption{The analyzed algorithms properties.}
\label{table:algo_descr}

\end{table}


\paragraph{Related works.}
The papers~\cite{random1,random2} proposed hyperparameters optimization strategies based on the random search. The current gold-standard methods~\cite{probopt1,probopt2}  proposed hyperparameter optimizations based on the probabilistic models construction for the optimal hyperparameter value prediction. These methods are  ineffective whenever the number of hyperparameters is large~\cite{hyper}.

The gradient-based hyperparameter optimization methods were suggested in~\cite{hyper,hyper2,hyper_mad,hyper_hoag,hyper_greed}. The papers~\cite{hyper_mad,hyper} proposed the gradient-based methods that use reverse differentiation method. This method is similar to backpropagation. The main idea of this approach is to restore the full history of parameter updates in order to optimize hyperparameters using this history. In general this procedure is computationally expensive. The authors proposed to use only the last update of the parameters at each optimization iteration in~\cite{hyper_greed} . This method can be considered as a greedy hyperparameter optimization algorithm. The authors optimized the model parameters using stochastic gradient descent with momentum in~\cite{hyper}. This allowed to use less memory for storing the parameter update history. The authors of the paper~\cite{hyper_mad} proposed to linearize the history trajectory to effectively restore the update history. The authors of the paper~\cite{hyper_hoag} used an approximation of the exact hyperparameter gradient, which can be calculated when the problem satisfies some regular conditions.

For the hyperparameter optimization various model selection criteria can be used~\cite{MacKay,Bishop}. One of them is the marginal likelihood or \textit{evidence}~\cite{MacKay,Bishop,tokmakova,strijov_dsc}. The authors  of the papers~\cite{tokmakova,strijov_dsc}  considered the problem of model selection and hyperparameter optimization for the linear regression problem. One of the marginal likelihood approximation methods is an \textit{evidence lower bound}, which can be obtained using variational inference~\cite{Bishop}. A stochastic version of the variational inference was proposed in~\cite{hoffman}. The authors of the paper~\cite{varmc} analyzed the relation between the gradient-based evidence lower bound estimations and MCMC methods. An alternative model selection criterion is a minimum description length~\cite{mdl} that characterizes statistical complexity of the model. An evidence lower bound estimation method for the neural networks was proposed in~\cite{nips}. The authors analyzed the relation between the evidence lower bound and minimum description length. 

The other model selection criterion is the cross-validation~\cite{cv_ms,tokmakova}. One of this criterion problem  is its high computational cost~\cite{expensive,expensive2}. The authors of the paper~\cite{bias2} analyzed the variance of cross-validation model performance estimation. In~\cite{tokmakova} the authors compared hyperparameter values obtained during hyperparameter optimization using various model selection criteria.



\section{Problem statement}
\label{prob_stat}
There is given a dataset \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m.\end{equation} It contains the object matrix $[\mathbf{x}_1, \dots, \mathbf{x}_m]^\mathsf{T} =  \mathbf{X}$ and the label vector  $[y_1, \dots, y_m]^\mathsf{T} = \mathbf{y}$. The label  ${y_i}$ is in finite set, ${y}_i \in \mathbb{Y} = \{1, \dots, Z\}$, in case of $Z$-class classification problem. For the regression problem the label $y_i$ is in the real-value subset ${y_i} \in \mathbb{Y} \subset  \mathbb{R}$.

A model $\mathbf{f}(\mathbf{w}, \mathbf{x})$ predicts the variable~$y_i$:
\[
	f:\mathbb{R}^u \times \mathbb{R}^n \to \mathbb{Y}, \quad \mathbf{w} \in \mathbb{R}^u.
\]
It is differentiable with respect to parameters $\mathbf{w}$.

Introduce the probabilistic interpretation of the model ${f}(\mathbf{w}, \mathbf{x})$ for the classification and regression problems.

\paragraph{Regression problem.}
Suppose the dependent variable $y$ is normally distributed:
\begin{equation}
\label{eq:reg}
\mathbf{y} = \mathcal{N}\bigl(\mathbf{f}, \mathbf{I}\bigr),\quad \text{where }\mathbf{f} = \mathbf{f}(\mathbf{w}, \mathbf{X}) = [f(\mathbf{w}, \mathbf{x}_1),\dots,f(\mathbf{w}, \mathbf{x}_m)]^\mathsf{T}.
\end{equation}


Define the likelihood function $p(\mathbf{y}|\mathbf{X}, \mathbf{w})$:
\[
	\text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}) =-\frac{m}{2}\text{log}(2\pi)   -\frac{1}{2}\bigl(\mathbf{y} - \mathbf{f}(\mathbf{w}, \mathbf{X}))^\mathsf{T}\mathbf{I}(\mathbf{y} - \mathbf{f}(\mathbf{w}, \mathbf{X})\bigr).
\] 

\paragraph{Classification problem.}
For the 2-class classification problem suppose the dependent variable $y$  is distributed binomially:
\begin{equation}
\label{eq:cl}
	\mathbf{y} = \mathcal{B}\bigl(1-\mathbf{f}, \mathbf{f}\bigr),
\end{equation}
where  $\mathbf{f}$ is the probability that objects from the matrix $\mathbf{X}$  belong to the first class.
For the multiclass classification problem the  dependent variable $y$  is distributed multinomially and $r$-th component ${f}_r$ is the probability that objects from the matrix $\mathbf{X}$  belong to the class $r$. Define the likelihood function
\[
	\text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}) = \sum_{\mathbf{x}, y \in \mathbf{X}, \mathbf{y}} \sum_{r=1}^Z[y=r] \text{log}{f}_r(\mathbf{w}, \mathbf{x}).
\] 

For both classification~\eqref{eq:cl} and regression~\eqref{eq:reg} problems define the prior distribution~$p(\mathbf{w}|\mathbf{A})$:
\begin{equation}
\label{eq:prior}
	\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1}),
\end{equation}
where  $\mathbf{A}^{-1} = \text{diag}[\alpha_1, \dots, \alpha_u]^{-1}$ is the covariance  diagonal matrix. The hypotheses~\eqref{eq:reg},~\eqref{eq:cl} and~\eqref{eq:prior} 	do	not	contradict	each other since the normal distribution is unbounded~\cite{bayes_constr}.  

In order to describe the general scheme of the model optimization procedure denote a vector of model parameters by $\mathbf{w}$. Denote by $\boldsymbol{\theta} \in \mathbb{R}^s$ a vector of parameters of the model parameters distribution. In the simple case $\boldsymbol{\theta}$ is a concatenation of vectors of parameters of multiple model instances. The details of $\boldsymbol{\theta}$ construction are given in the model selection methods description below. 

Introduce an example of hyperparameter optimization using the coherent Bayesian inference. Optimize model parameters $\boldsymbol{\theta} = \mathbf{w}$ according to the model parameter distribution $p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A})$:
\begin{equation}
\label{eq:bayes1}
\hat{\boldsymbol{\theta}} = \argmax \bigl(-L(\boldsymbol{\theta}, \mathbf{A})\bigr) = p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A}) = \frac{p(\mathbf{y}|\mathbf{X},\mathbf{w})p(\mathbf{w}|\mathbf{A})}{p(\mathbf{y}|\mathbf{X},\mathbf{A})}.
\end{equation}

Calculate the hyperparameter  $\mathbf{A}$ posterior distribution:
\[
p(\mathbf{A}|\mathbf{X}, \mathbf{y}) \propto p(\mathbf{y}|\mathbf{X},\mathbf{A})p(\mathbf{A}),
\]
where $\propto$ means proportionality.

Suppose $p(\mathbf{A})$ is uniform on a large interval. The hyperparameter optimization problem is:
\begin{equation}
\label{eq:bayes2}
	Q(\boldsymbol{\theta}, \mathbf{A}) = p(\mathbf{y}|\mathbf{X},\mathbf{A}) = \int_{\mathbf{w} \in \mathbb{R}^u} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{A}) \to \max_{\text{diag}(\mathbf{A}) = [\alpha_1, \dots, \alpha_u] \in \mathbb{R}^{n}}.
\end{equation}


%\begin{figure}
%  \includegraphics[width=0.8\linewidth]{slide_hyper.png}
%\label{fig:hyper}
%    \caption{Marginal likelihood dependency on hyperparameter $\alpha$ disrtribution.}
 
   
%    \end{figure}


Formulate the hyperparameter optimization problem. Given functions $L$ and $Q$ that characterize the model selection method. The loss function $L$ optimizes parameters $\boldsymbol{\theta}$. The validation function $Q$ evaluates the model predictive performance.
The problem is to optimize the parameters $\boldsymbol{\theta}$ and the hyperparameters $\mathbf{A}$: 
\begin{equation}
\label{eq:main}
	\hat{\mathbf{A}} = \argmax_{[\alpha_1, \dots, \alpha_u] \in \mathbb{R}^n} Q(\hat{\boldsymbol{\theta}}(\mathbf{A}), \mathbf{A}),
\end{equation}
\begin{equation}
\label{eq:main2}
	\hat{\boldsymbol{\theta}}(\mathbf{A}) =  \argmin_{\boldsymbol{\theta} \in \mathbb{R}^s} L(\boldsymbol{\theta}, \mathbf{A}).
\end{equation}

Introduce the functions $L, Q$ and $\boldsymbol{\theta}$ for various model selection methods.

\paragraph{Basic method.}
Optimize the model parameters $\boldsymbol{\theta}$ and hyperparameters $\mathbf{A}$ using the whole dataset $\mathfrak{D}$ and the same function for both optimization and model evaluation with one model instance, $L = -Q$:
\begin{equation} 
\label{eq:basic}
Q(\boldsymbol{\theta}, \mathbf{A}) = -L(\boldsymbol{\theta}, \mathbf{A})  = \text{log}p(\mathbf{y}, \mathbf{w} | \mathbf{X}, \mathbf{A}) = \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w})+\text{log}p(\mathbf{w}|\mathbf{A}).
\end{equation}

Describe two model selection methods that prevent model overfitting.

\paragraph{Cross-validation~\cite{cv_ms}.}
Split the dataset  $\mathfrak{D}$ into $k$ equal parts:
$
\mathfrak{D} = \mathfrak{D}_1 \sqcup \dots \sqcup \mathfrak{D}_k.
$


Run $k$ model optimizations for each subset. The vector $\boldsymbol{\theta}$ is a concatenation of model parameters $\mathbf{w}_k$ for each optimization instance:
\[
\boldsymbol{\theta} = [\mathbf{w}_1, \dots, \mathbf{w}_k].
\]
 
Construct the loss function $L$ as an average negative log posterior probability over the remaining part of each split $\mathfrak{D}_1, \dots, \mathfrak{D}_k$:  
\begin{equation}
\label{eq:cv}
L(\boldsymbol{\theta}, \mathbf{A}) = -\frac{1}{k}\sum_{c=1}^k \bigl(\frac{k}{k-1}\text{log}p(\mathbf{y} \setminus \mathbf{y}_c|\mathbf{X}\setminus \mathbf{X}_c, \mathbf{w}_c) + \text{log}p(\mathbf{w}_c|\mathbf{A})\bigr).
\end{equation}

Construct the validation function $Q$ as an average log likelihood over $k$ splits:
\begin{equation}
\label{eq:cvq}
Q(\boldsymbol{\theta}, \mathbf{A}) = \frac{1}{k}\sum_{c=1}^k k\text{log}p(\mathbf{y}_c|\mathbf{X}_c, \mathbf{w}_c).
\end{equation}

\paragraph{Evidence Lower Bound~\cite{nips}.}
As in the basic method description let $L=-Q$. It is an evidence lower bound:
\begin{equation} 
\label{eq:elbo}
\text{log}~p(\mathbf{y}|\mathbf{X},\mathbf{A})  
\geq 
-\text{D}_\text{KL} \bigl(q(\mathbf{w})||p(\mathbf{w}|\mathbf{A})\bigr) + \int_{\mathbf{w}} q(\mathbf{w})\text{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{w},\mathbf{A})} d \mathbf{w}  \approx
\end{equation}
\[
\approx \sum_{i=1}^m \text{log}~p({y}_i|\mathbf{x}_i, \mathbf{w}_i) - D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{A})\bigr) = -L(\boldsymbol{\theta}, \mathbf{A}) = Q(\boldsymbol{\theta}, \mathbf{A}),
\]
where $q$ is an auxiliary distribution called variational distribution. Let $q$ be a normal distribution:
\begin{equation}
\label{eq:diag}
	q \sim \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q),
\end{equation}
where $\mathbf{A}_q = \text{diag}[\alpha^q_1, \dots, \alpha^q_u]^{-1}$ is  a diagonal covariance matrix and $\boldsymbol{\mu}_q$ is a mean vector.
The divergence $D_\text{KL}$ between 2 Gaussian variables is 
\[
	D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{f})\bigr) = \frac{1}{2} \bigl( \text{Tr} [\mathbf{A}\mathbf{A}^{-1}_q] + (\boldsymbol{\mu} - \boldsymbol{\mu}_q)^\mathsf{T}\mathbf{A}(\boldsymbol{\mu} - \boldsymbol{\mu}_q) - u +\text{ln}~|\mathbf{A}^{-1}| - \text{ln}~|\mathbf{A}_q^{-1}| \bigr).
\]
The vector of optimized parameters $\boldsymbol{\theta}$ is a vector of variational distribution $q$ parameters:
\[
\boldsymbol{\theta} = [\text{diag}(\mathbf{A}_q), {\mu}_1,\dots,{\mu}_u].
\]




\section{Gradient-based hyperparameter optimization methods}
\label{gbhom}
In this section we analyze hyperparameter optimization methods based on gradient descent. Suppose the parameters $\boldsymbol{\theta}$ are also optimized using gradient-based methods. 

%\newdefinition{dfn}{Definition}
\begin{definition}
A stochastic gradient descent operator $T$ estimates the parameters $\boldsymbol{\theta}^\prime$ using their previous values~$\boldsymbol{\theta}$ with random subset of dataset:
\[
	\boldsymbol{\theta}^\prime = T(\boldsymbol{\theta}, \mathbf{A}, \mathfrak{D}) = \boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A})_{\mathfrak{D} = \hat{\mathfrak{D}}},
\]
where $\hat{\mathfrak{D}}$ is a random subset of $\mathfrak{D}$.
\end{definition}

Optimize the parameters  $\boldsymbol{\theta}$ with $\eta$ steps of stochastic gradient descent:
\begin{equation}
\label{eq:gd}
	 \hat{\boldsymbol{\theta}} = T \circ T \circ \dots \circ T(\boldsymbol{\theta}_0, \mathbf{A}) = T^\eta(\boldsymbol{\theta}_0, \mathbf{A}), \quad 	T(\boldsymbol{\theta}, \mathbf{A}) =\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A}), 
\end{equation}
where $\gamma$ is the learning rate, $\boldsymbol{\theta}_0$ is the initial values of the vector $\boldsymbol{\theta}$. 

Redefine the optimization problem according to the definition of operator $T$:
\begin{equation}
\label{eq:optim}
	\hat{\mathbf{A}} = \argmax_{\text{diag}(\mathbf{A}) \in \mathbb{R}^n} Q\bigl( T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr).
\end{equation}

The general scheme of the hyperparameter optimization is the following:
\begin{enumerate}
\item in range from 1 to  $l$, where $l$ is the number of the hyperparameter optimization iterations:
\item initialize parameters $\boldsymbol{\theta}$;
\item solve the optimization problem~\eqref{eq:optim} and obtain the new hyperparameter values $\hat{\mathbf{A}}$;
\item set $\mathbf{A} = \hat{\mathbf{A}}$.
\end{enumerate}

One of the methods for the problem~\eqref{eq:optim} solution is to use a gradient descent not only for the parameters $\boldsymbol{\theta}$, but also for the hyperparameters $\mathbf{A}$. 
The exact gradient calculation $\nabla_{\mathbf{A}} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{A}))$ is intractable because of the inner optimization operator $T$. Therefore the number of approximated solutions were presented. Their main properties are listed in Table~\ref{table:algo_descr2}. An example of the parameter and hyperparameter update with different hyperparameter optimization algorithms is illustrated in Fig.~\ref{fig:traj}.

\begin{table}
\small

\begin{tabularx}{\textwidth}{|X|X|X|X|}
\hline
\bf Algorithm & \bf Type & \bf Optimization iteration complexity & \bf Correctness suppositions  \\ 
\hline
Random search & stochastic & $O(\eta s |\hat{\mathfrak{D}}|)$& -  \\ \hline
Greedy~\cite{hyper_greed} & gradient-based & $O(\eta us |\hat{\mathfrak{D}}|)$ & $\mathbf{H}(\boldsymbol{\theta}) = \mathbf{I}$  \\ \hline
HOAG~\cite{hyper_hoag} & gradient-based & $O(\eta us |\hat{\mathfrak{D}}| + \text{o}),$ where $o$ is a complexity of linear equation solution& first derivatives of $Q$  and second derivatives of $L$ are Lipshitz functions;  $\text{det}\mathbf{H} \neq \mathbf{0}$;  \\ \hline
DrMAD~\cite{hyper_mad} & gradient-based &$O(\eta us |\hat{\mathfrak{D}}|)$ & Parameter trajectory~$\boldsymbol{\theta}$ = $\boldsymbol{\theta}_0, \dots \boldsymbol{\theta}_\eta$  is linear \\ \hline
\end{tabularx}

\caption{The analyzed algorithms complexity and correctness. The model is a multilayer perceptron optimized using backpropagation with the basic model selection method~\eqref{eq:basic}.}
\label{table:algo_descr2}

\end{table}
%Greedy: backprop + optimization. 


\paragraph{Greedy algorithm.}
Update the hyperparameter $\mathbf{A}$ using gradient descent, which depends only on the last update of parameters $\boldsymbol{\theta}$. Optimize the hyperparameters and parameters in a single optimization procedure. Update the hyperparameter $\mathbf{A}$ at every iteration:
\[
	\hat{\mathbf{A}} = \mathbf{A} + \gamma_{\mathbf{A}} \nabla_{\mathbf{A}}  Q \bigl(T(\boldsymbol{\theta}, \mathbf{A}) , \mathbf{A}\bigr) = \mathbf{A} + \gamma_{\mathbf{A}} \nabla_{\mathbf{A}}  Q\bigl(\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A}), \mathbf{A})\bigr),
\]
where $\gamma_{\mathbf{A}}$ is the learning rate for the hyperparameter optimization.

\paragraph{HOAG.}
Obtain approximate hyperparameter gradient values  $\nabla_{\mathbf{A}} Q \bigl(T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr)$ using the following  formula approximation of the  of the gradient:
\[
\nabla_{\mathbf{A}} Q \bigl(T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr) = \nabla_{\mathbf{A}} Q(\boldsymbol{\theta}, \mathbf{A}) - (\nabla_{ \mathbf{A}}  \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \mathbf{A}))^\mathsf{T}\mathbf{H}(\boldsymbol{\theta})^{-1}\nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{A}),
\]
where $\mathbf{H}$ is the Hessian of the function $L$ with respect to the parameters $\boldsymbol{\theta}$.

The algorithm of hyperparameter gradient $\nabla_{\mathbf{A}} Q$  approximation:
\begin{enumerate}
\item Optimize the parameters $\boldsymbol{\theta} = T^\eta(\boldsymbol{\theta}_0, \mathbf{A})$.
\item Solve linear system for a vector $\boldsymbol{\lambda}$: $\mathbf{H}(\boldsymbol{\theta})\boldsymbol{\lambda} =  \nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{A})$.
\item Compute the approximate hyperparameter gradient: $\hat{\nabla}_{\mathbf{A}}Q = \nabla_{\mathbf{A}}Q(\boldsymbol{\theta}, \mathbf{A}) - \nabla_{ \mathbf{A}}  \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \mathbf{A})^\mathsf{T}\boldsymbol{\lambda}$.
\end{enumerate}

The final update rule is
\begin{equation}
\label{eq:update_hyper}
\hat{\mathbf{A}} = \mathbf{A} + \gamma_{\mathbf{A}} \hat{\nabla}_{\mathbf{A}}Q.
\end{equation}
The computational cost of the Hessian $\mathbf{H}(\boldsymbol{\theta})$ evaluation  from step 2 is high.  In this paper we use stochastic gradient descent for solving this linear system. 


\paragraph{DrMad.}
For the hyperparameter gradient evaluation restore the full history of $\eta$ parameters updates starting from the initial value $\boldsymbol{\theta}_0$.  For the simplification of this procedure suppose that the trajectory of parameters $\boldsymbol{\theta}$ update is linear:
\begin{equation}
\label{eq:mad_lin}
\boldsymbol{\theta}^\tau = \boldsymbol{\theta}_0 + \frac{\tau}{\eta} T(\boldsymbol{\theta}).
\end{equation}

The algorithm of approximate hyperparameter gradient calculation:
\begin{enumerate}
\item Optimize  parameters $\boldsymbol{\theta} = T^\eta(\boldsymbol{\theta}_0, \mathbf{A})$.
\item Let $\hat{\nabla} \mathbf{A} = \nabla_\mathbf{A} Q(\boldsymbol{\theta}, \mathbf{A}).$ 
\item Let $\hat{\nabla} \boldsymbol{\theta} = \nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{A}).$ 
%\item Let $d\mathbf{v} = \mathbf{0}.$
\item In range from $\tau = \eta$ to  1:
\item Compute $\boldsymbol{\theta}^\tau$\eqref{eq:mad_lin}.
\item $d\mathbf{v} =  \gamma \hat{\nabla} {\boldsymbol{\theta}}$.
\item $\hat{\nabla} \mathbf{A} =  \hat{\nabla} \mathbf{A} + d\mathbf{v}\nabla_{\mathbf{A}} \nabla_{\boldsymbol{\theta}} L$.
\item $\hat{\nabla} \boldsymbol{\theta}  = \hat{\nabla} \boldsymbol{\theta}  - d\mathbf{v}\nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} L$.
\end{enumerate}
The final update rule is analogous to~\eqref{eq:update_hyper}.
The algorithm is instable when the learning rate $\gamma$ is large~\cite{hyper_mad}. For the stability of the algorithm we discard first 5\% of values  $\boldsymbol{\theta}$ and calculate only each $\tau_\text{step}$ step of parameter updates history:
\begin{equation}
\label{eq:mad_lin2}
\boldsymbol{\theta}^\tau = \boldsymbol{\theta}_{\tau_0} + \frac{\tau}{\eta} T(\boldsymbol{\theta}), \quad \tau \in \{\tau_0, \tau_o + \tau_\text{step}, \dots,\eta - \tau_\text{step}, \eta\},
\end{equation}
where $\tau_0 = \ceil{0.05 \cdot \eta}$.

\section{Experiments}
\label{experiments}
For the evaluation of the analyzed algorithms we conduct a series of computational experiments. A synthetic dataset~\cite{pyfos}, WISDM~\cite{wisdm} and MNIST~\cite{mnist} datasets are used. All datasets are split into Train and Test subsets. The algorithms are optimized on the Train subsets and evaluated on the Test subsets. For each dataset and each hyperparameter optimization algorithm we run the optimizations 5 times. The results are averaged. 
We use the following evaluation criteria.
\begin{enumerate}
\item Quality: the best value of $Q$:  $\hat{Q} = \max_{j \in \{1, \dots, l\}}Q^j$.
\item Convergence: the number of iterations to have a validation value greater than 99\% of the best value $\hat{Q}$:
\[
    \argmin_{j}: \frac{Q^j - Q^0}{\hat{Q} - Q^0} \geq 0.99,
\]
where $Q^0$ is the value of $Q$ before the hyperparameter optimization. 


\item Error function $E$:\[
    E = \text{RMSE}(\mathbf{X}_\text{test}, \mathbf{y}_\text{test}) = \left (\frac{1}{|\mathbf{X}_\text{test}|}\sum_{\mathbf{x} \in\mathbf{X}_\text{test}, y \in \mathbf{y}_\text{test}}  (f(\mathbf{x}, \mathbf{w})-y)\right)^{\frac{1}{2}}
\]
for the regression problem and
\[
    E = \text{Acc}(\mathbf{X}_\text{test}, \mathbf{y}_\text{test}) = 1 - \frac{1}{|\mathbf{X}_\text{test}|}\sum_{\mathbf{x} \in\mathbf{X}_\text{test}, y \in \mathbf{y}_\text{test}} [f(\mathbf{x}, \mathbf{w}) \neq y]
\]
for the classification problem.


\item The error function  $E_\sigma$ for the model with noisy dataset:
\[
    \text{RMSE}_\sigma =  \text{RMSE}(\mathbf{X}_\text{test} +  \boldsymbol{\varepsilon}, \mathbf{y}_\text{test}),\]\[\text{Acc}_\sigma =  \text{Acc}(\mathbf{X}_\text{test} +  \boldsymbol{\varepsilon}, \mathbf{y}_\text{test}),  \quad \boldsymbol{\varepsilon} \sim \mathcal{N}\bigl(\mathbf{0}, \sigma\mathbf{I}\bigr).
\]
\end{enumerate}



The random search is used as a baseline. We set the number of the random search iterations to the number $l$ of gradient-based hyperparameter optimization algorithms iterations. It is set to $l=50$ for the synthetic dataset and WISDM dataset, $l=25$ for the MNIST dataset. We use two types of loss function $L$ and validation function $Q$:
\begin{enumerate}
\item evidence lower bound for both $L$ and $Q$~\eqref{eq:elbo};
\item cross-validation functions~\eqref{eq:cv},\eqref{eq:cvq}.
\end{enumerate}

For the neural network models we set all the hyperparameters equal for each layer when use  cross-validation~\eqref{eq:cv}. We use the full diagonal parameterization of the hyperparameters for all the linear models and for the neural network models when we optimize evidence lower bound~\eqref{eq:elbo}.


\begin{figure*}[tbh!]
    \centering
    \includegraphics[width=0.5\linewidth]{Fig_traj2.eps}

    \caption{An example of parameter update trajectories. The color displays the value of the validation function $Q$. Greedy algorithm optimizes hyperparameter during the parameter optimization, therefore it has the light blue trajectory between the optimized green trajectory of HOAG algorithm and the dark blue trajectory of parameters without hyperparameter optimization. DrMAD uses a linearized dashed parameter trajectory during hyperparameter optimization procedure.  }
    \label{fig:traj}
    \end{figure*}


The hyperparameters are initialized with uniform distribution $\mathcal{U}(a,b)$,
where $a = -2, b = 10$ for the synthetic dataset and $a = -4, b = 10$  for the WISDM and MNIST datasets.

We calibrate the hyperparameter learning rate $\gamma_{\mathbf{A}}$  for each algoritm using grid search:~$\{r \cdot 10^{s}, s \leq 1, r \in \{1,25,50,75\}\}$. The largest value of $\gamma_{\mathbf{A}}$ is chosen if the final hyperparameter value $\mathbf{A}$ satisfied the condition:
\[
    a_\text{min} \leq  \min(\mathbf{A}), \quad \max(\mathbf{A}) \leq b_\text{max},
\] 
where $a_\text{min} = -2.5, b_\text{max}=10.5$ for the synthetic dataset and $a_\text{min} = -5, b_\text{max}=11$ for the WISDM and MNIST datasets. We calibrate $\gamma_\mathbf{A}$ with a small amount of iterations: $l=50$ for the synthetic dataset, $l=10$ for the WISDM and $l=5$ for the MNIST dataset. Whenever each algorithm shows instability in further experiments (gradient explosion or overflow) we lower the value of $\gamma_{\mathbf{A}}$. The parameter $\tau_\text{step}$ is set to $1$ for synthetic dataset and WISDM and  $\tau_\text{step}=10$ for the MNIST dataset.
The overall results are presented in Table~\ref{table:table}. The implementation of the proposed algorithms is available at~\cite{pyfos}.


\paragraph{Synthetic dataset.}
The synthetic dataset is generated according to the rule:
\[
	\mathbf{y} = \mathbf{x} + \boldsymbol{\varepsilon},\quad \mathbf{x}  \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
\]
where $\quad m = 40, \quad n = 1.$

The regression model $\mathbf{f}$ uses the following features: $\{\mathbf{x}^0, \dots, \mathbf{x}^9, \textbf{sin}(\mathbf{x}), \textbf{cos}(\mathbf{x})\}$. The unregularized regression model with such ratio of  object number and feature number tends to overfit significantly. The goal of this experiment is to analyze if the hyperparameter optimization could prevent such models from overfitting.

Fig.~\ref{fig:poly} plots the resulting polynomials.  Since the model is overparametrized, the optimization without any overfitting control tends to overfit drastically. The real dependency between $\mathbf{y}$ and $\mathbf{X}$ is linear. As we can see, the evidence lower bound criterion~\eqref{eq:elbo} gives non-overfitted polynomials, which parameters are close to the linear models.

\paragraph{WISDM.}
The WISDM dataset contains a set of records from accelerometer. Each record has three coordinates that correspond to the accelerometer axes. As a set of features we use first 199  records from each 200 sequential records. We use $l_2$ norm of the 200th records as a label and required to predict. 

A neural network with 10 neurons on the hidden layer is used:
\[
    \mathbf{f} = \mathbf{W}_2 \cdot \textbf{RELU}(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) +\mathbf{b}_2,
\]
where $\mathbf{W}_1, \mathbf{b}_1$ --- hidden layer parameters,
$\mathbf{W}_2, \mathbf{b}_2$ --- output layer parameters,
\[
    \textbf{RELU}(\mathbf{x}) = \max(\mathbf{0}, \mathbf{x}).
\]

Fig.~\ref{fig:wisdm} shows the plots with RMSE and best validation loss value $\hat{Q}$ for the WISDM dataset.
DrMad and HOAG algorithms shows significantly lower results than greedy algorithm. The random search shows good results in case of cross-validation, when the number of hyperparameters is small. When the number of hyperparameters is large the greedy and HOAG algorithm shows good results. HOAG requires more iteration number $l$ than the greedy algorithm for the convergence.


\paragraph{MNIST.}
The MNIST dataset contains a set of images of handwritten digits. 
We use a neural network with 300 neurons on the hidden layer and softmax output.


Fig.~\ref{fig:mnist} plots the error $E$ and quality $\hat{Q}$.
For the evidence lower bound criterion, the models with highest $\hat{Q}$ value have also highest error $E$. Nevertheless, 
these models show the best results for the stability, when the noise in dataset is high. Fig.~\ref{fig:noise} plots  the error $E_\sigma$. Models with highest evidence lower bound have the lowest error when the Test dataset is noisy. We interpret this as an ability to select the model with highest generalization ability.

The experiments show that the gradient-based methods give better results than the random search when the number of hyperparameters is large. The best results were obtained by the greedy algorithm. We find that the hyperparameter optimization is instable when using DrMad algorithm, therefore its learning rate  $\gamma_{\mathbf{A}}$ is set to low values.




\begin{table}
\footnotesize
\begin{tabularx}{\textwidth}{ |X|p{0.75cm}|X|X|p{1cm}|X|X|X|X|}

\hline
\textbf{Algorithm} & $L, Q$  & $Q(\boldsymbol{\theta}, \mathbf{A})$ & Convergence & E & $E_{0.25}$ & $E_{0.5}$\\ 
\hline
\multicolumn{7}{|c|}{\textit{Synthetic}}  \\
\hline
Random search & ~\eqref{eq:cv} & \bf -171.6  &\bf 26.2 $\pm$ 20.0  & \bf 1.367 &\bf 1.410 &\bf 1.555 \\
\hline
Greedy & ~\eqref{eq:cv} & -172.5 & 30.0 $\pm$ 24.5 & 1.421 & 1.439 &  1.536\\
\hline
DrMAD & ~\eqref{eq:cv} & -174.1 & 40.2 $\pm$ 16.1 &  1.403 & 1.424 & \bf 1.512 \\
\hline
HOAG & ~\eqref{eq:cv} &-174.7 & 29.4 $\pm$ 24.0 &   \bf 1.432  & 1.463 & 1.553\\
\hline
Random Search & ~\eqref{eq:elbo} & -63.5  & 32.4 $\pm$ 18.7  & 1.368 & 1.426 & 1.546  \\
\hline
Greedy & ~\eqref{eq:elbo} & -25.5 & \bf 1.2 $\pm$ 0.4 & 1.161 & 1.174 & 1.193\\
\hline
DrMAD & ~\eqref{eq:elbo} & \bf -25.1 &  10.6 $\pm$ 0.8 &  1.157 & 1.163 &  1.184\\
\hline
HOAG & ~\eqref{eq:elbo} &-25.8 & 10.8 $\pm$ 1.5&   \bf 1.141  & \bf 1.149 & \bf 1.177\\
\hline


\multicolumn{7}{|c|}{\textit{WISDM}}  \\
\hline
Random search & ~\eqref{eq:cv} & \bf -1086661.1  & 22.0 $\pm$ 19.3  & \bf 0.660 & \bf 0.670 & \bf 0.690  \\
\hline
Greedy & ~\eqref{eq:cv} & -1086707.1 & \bf 15.4 $\pm$ 17.2 & 0.707 &  0.723  &  0.769\\
\hline
DrMAD & ~\eqref{eq:cv} & -1086708.2 & 29.2 $\pm$ 8.0 &  0.694 &  0.708 & 0.742 \\
\hline
HOAG & ~\eqref{eq:cv} & -1086733.5 & 28.2 $\pm$ 7.13&   0.701 & 0.724 & 0.753 \\
\hline
Random search & ~\eqref{eq:elbo} & -35420.4 &   14.4 $\pm$ 7.8  &   0.732 &   0.755 & 0.785 \\
\hline
Greedy & ~\eqref{eq:elbo} & \bf -3552.9 &\bf 1.0 $\pm$ 0.0  &   \bf 0.702 & \bf 0.730  &  \bf 0.767\\
\hline
DrMAD & ~\eqref{eq:elbo} & -26091.4 &   50.0 $\pm$ 0.0  & 0.729 &  0.753 & 0.816 \\
\hline
HOAG & ~\eqref{eq:elbo} &  -16566.6 & 49.0 $\pm$ 0.0  &  0.733 &  0.755 &  0.801 \\
\hline



\multicolumn{7}{|c|}{\textit{MNIST}}  \\
\hline
Random search & ~\eqref{eq:cv} & -3236.4  & 7.8 $\pm$ 1.9  &   0.981 & \bf 0.966 & \bf 0.866 \\
\hline
Greedy & ~\eqref{eq:cv} & \bf -3416.7 & 10.8 $\pm$ 10.4 & 0.979 & 0.962 & 0.860\\
\hline
DrMAD & ~\eqref{eq:cv} & -3469.0 & 17.0 $\pm$ 5.6 & \bf  0.982 & 0.962 & 0.831\\
\hline
HOAG & ~\eqref{eq:cv} & -3748.6 & \bf 8.6 $\pm$ 7.3&   0.980 &  0.961  & 0.853 \\
\hline
Random search & ~\eqref{eq:elbo} & -1304556.4 &  14.2 $\pm$ 5.7 &  \bf 0.982 & 0.943 & 0.814 \\
\hline
Greedy & ~\eqref{eq:elbo} & \bf -11136.2 & \bf 1.0 $\pm$ 0.0  &  0.977 & \bf 0.952 & \bf 0.884\\
\hline
DrMAD & ~\eqref{eq:elbo} & -1305432.9 & 24.6 $\pm$ 0.5  & \bf 0.982 & 0.941 & 0.813 \\
\hline
HOAG & ~\eqref{eq:elbo} &  -280061.6 & 24.0 $\pm$ 0.0  & 0.981 & 0.943 & 0.819\\
\hline


\hline
\end{tabularx}
\caption{Experiment results}
\label{table:table}
\end{table}

    \begin{figure*}

    \begin{subfloat}[]{
    \includegraphics[width=0.5\linewidth]{Fig_poly_cv2.pdf}
    }
    \end{subfloat}
    \begin{subfloat}[]{
    \includegraphics[width=0.5\linewidth]{Fig_poly_var2.pdf}
    }
    \end{subfloat}

 \caption{Resulting models for the synthetic dataset: a --- cross-validation, b --- evidence lower bound}
  \label{fig:poly}
   
    \end{figure*}




    \begin{figure*}

    \includegraphics[width=\linewidth]{Fig_wisdm2.eps}
\caption{WISDM,  best validation value $\hat{Q}$ and RMSE  for cross-validation (left) and evidence lower bound (right)}    
\label{fig:wisdm}
    
    \end{figure*}


    \begin{figure*}

    \includegraphics[width=\linewidth]{Fig_mnist2.eps}

    \caption{MNIST, best validation value  $\hat{Q}$ and Accuracy  for cross-validation (left) and evidence lower bound (right)}
    \label{fig:mnist}
    \end{figure*}

    \begin{figure*}

    \includegraphics[width=\linewidth]{Fig_noise2.pdf}

    \caption{MNIST, Model accuracy with noise in the Test dataset. The hyperparameters were optimized with evidence lower bound criterion.}
    \label{fig:noise}
    \end{figure*}





\section{Discussion}
\label{discussion}
The experiments show that each algorithm perform effectively and therefore the appropriate hyperparameter optimization method should rely on the amount of hyperparameters and the specific of the problem. 

When dealing with small amount of hyperparameters random search shows the best results since search procedure can be employed more effectively than gradient-based optimization in low-dimensional hyperparameters space. For the high-dimensional hyperparameter space both  HOAG and greedy algorithms show good performance. 
HOAG algorithm is more preferable if the model optimization problem is expensive. On the other hand we can schedule greedy hyperparameter optimization to make it less expensive as in~\cite{hyper_greed}. 

DrMad algorithm showed rather poor results on the MNIST and WISDM datasets. Perhaps, it is so because of high learning rate $\gamma$ used in experiments. The large value of the learning rate can make DrMad algorithm instable. Two improvements can be proposed. We can use more stable optimization like Adam or AdaGrad for both parameter and hyperparameter optimization. The second improvement was proposed in~\cite{hyper_mad}: we can use more complicated parameter trajectory approximation to make it more similar to the original parameter trajectory. Opposing to HOAG and greedy algorithms, DrMad optimization has prerequisites for not only hyperparameters optimization but also the metaparameters, i.e., the optimization procedure parameters. The opportunity of such optimization using reversed differentiation was shown in~\cite{hyper_mad}. 

The other interesting aspect of our experiments is the relation between the model error (RMSE or Accuracy) and the value of validation loss $Q$. The models obtained by the evidence lower bound showed higher error rate than the models obtained using cross-validation on the MNIST and WISDM datasets. These models also show greater stability when noise was added to the Test datasets.  The evidence lower bound showed significantly better results on the synthetic dataset, when the amount of objects in the Train dataset is small. Therefore we conclude that the evidence lower bound usage is preferable when the model tends to overfit or when the cross-validation usage is too computationally expensive.   In~\cite{nips} it was noted that the evidence lower bound optimization required more iterations for the convergence. In our experiments we used the same number of iterations both for the cross-validation and evidence lower bound. The more accurate iteration number calibration can improve the final quality of these models. 

\section{Conclusion and future work}
\label{concl}
The paper analyzes the gradient-based hyperparameter optimization algorithms. We adapt the analyzed algorithms for general validation functions and evaluated their performance on the  MNIST and WISDM datasets. Two model selection criteria are compared: the cross-validation and evidence lower bound. 

The experiments show that the gradient-based algorithms are effective when the number of hyperparameters is large. The results show that models obtained using evidence lower bound have higher error rates than models obtained using cross-validation, but they are also more stable when the test dataset contains a lot of noise. 

The authors  implement these algorithms as a toolbox available at~\cite{pyfos}. The toolbox is developed in Python using Theano~\cite{theano} and Numpy~\cite{numpy} libraries. 

In out future work we are planning to develop the analyzed algorithm and to extend gradient-based algorithms to optimize not only hyperparameters, but also the model optimization parameters. The other object of our future research is the difference between the cross-validation and evidence lower bound and the theoretical aspects of their properties for the models with large amount of parameters.

%addcontentsline{toc}{section}{References}
\bibliographystyle{spmpsci}
%\bibliographystyle{unsrt}
\bibliography{lit}

\end{document}
