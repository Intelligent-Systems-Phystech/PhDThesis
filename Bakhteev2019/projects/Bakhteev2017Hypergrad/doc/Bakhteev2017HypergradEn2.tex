    \documentclass[12pt]{article}
\usepackage{cmap}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}

\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[russian, english]{babel}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{cite}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}

\usepackage{mathtools}
\sloppy
%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}
\title{Comprehensive analysis of gradient-based hyperparameter optimization algorithms}
\date{}
\maketitle

 \begin{center}
 {O.\,Y.~Bakhteev\footnote{Moscow Institute of Physics and Technology, bakhteev@phystech.edu},
 V.\,V.~Strijov$^1$\footnote{FRCCSC of the Russian Academy of Sciences, strijov@ccas.ru}} % основной список авторов, выводимый в оглавление
 \end{center}

\textbf{Abstract:} 
%The paper investigates the problem of hyperparameter optimization problem for models with large number of parameters.
%For this problem solution the paper proposes gradient-based algorithms. For the model selection propose probabilistic interpretation of parameter distribution. The paper compares two model selection criterion: cross-validation and evidence lower bound that is a lower bound of marginal likelihood. The experiment is conducted on a series of datasets.\bigskip

%The paper investigates the problem of hyperparameter optimization problem.
%The paper analyzes gradient-based algorithms of hyperparameter optimization: HOAG, DrMad and greedy algorithm.  The complexity of the parameter optimization is comparable to the hyperparameter optimization complexity, therefore  optimize the model parameters and hyperparameters is a single procedure. The paper compares two model selection criterion: cross-validation and evidence lower bound that is a lower bound of marginal likelihood. 

The paper investigates the problem of hyperparameter optimization. Hyperparameters are the parameters of model parameter distribtuion. The adequate choice of hyperparameter values allows model not to overfit and obtain higher predictive performance.  Neural network models with large amount of hyperparameters are analyzed. The hyperparameter optimization for the models is computationally expensive. The paper proposes modifications of various gradient-based methods to optimize many hyperparameters simultaneously. The paper compares the results with the random search. The main impact of the paper is the analysis of hyperparameter optimization algorithms for the models with the high amount of parameters. To select precise and stable models the authors sugest to use two model selection criteria: cross-validation and evidence lower bound. The algorithms are evaluated on  regression and classification datasets.
%Behaviour - разбить, как-то разбить (идеи: bewhaviour, small am,ount ,randopm search)
%Strijov - 1,2 
%
%TODO: lab


\textbf{Keywords}:  gradient descent, stochastic hyperparameter search, hyperparameter optimization, model selection, neural networks, classification, regression.
\section{Introduction}
The papers analyzes the hyperparameter optimization problem. The \textit{model} is a function superposition, which solves a classification or regression problem. The hyperparameters of the model are the parameters of the model parameter distribution.

The paper focuses on thr neural network models. The parameter optimization problem is computationally expensive because of non-convexity of training loss. The amount of model parameters can reach millions~\cite{hinton_rbm}  and the optimization of such models requires multiple days\cite{suts}.   The hyperparameter values can significantly influence the quality of the models~\cite{journal1,journal2}. These two facts make the hyperparameter optimization problem very important.

In this paper gradient-based algorithms are analyzed. They optimize a large amount of hyperparameters in time in contrast to gradient-free algorithms~\cite{hyper}.  The complexity of hyperparameter optimization is comparable to the model parameter optimization complexity, therefore the parameters and hyperparameters are  optimized in a single procedure. As a baseline a random search algorithm is used. The pros and cons of all the algorithms  
are illustrated in Table~\ref{table:algo_descr}. Opposing to papers~\cite{hyper_mad, hyper_hoag, hyper_greed} this research does not focus on the hold-out cross-validation and analyzes the algorithms in general. The analyzed algorithms are implemented as a toolbox~\cite{pyfos}. The authors propose a number of modification of analyzed algorithms: we extend all the algorithms to work with validation loss that can contain hyperparameters, for DrMad algorithm~\cite{hyper_mad} we add an additional parameter that regularizes a length of analyzed parameter update trajectory, for HOAG algorithm~\cite{hyper_hoag} we use stochastic gradient descent for large linear system solving because of high dimension of the linear system.   The main theoretical impact of the paper is the analyze of the algorithm behavior with various validation loss functions and analyze of quality and stability of the resulting models. The experiments were conducted using cross-validation and evidence lower bound as a model selection criterion. Opposing to the paper~\cite{hyper_hoag}, where also the optimization algorithms comparison can be found, this paper present results on the large datasets, such as  WISDM~\cite{wisdm} and MNIST~\cite{mnist}, where the number of hyperparameters is significantly large. 
The experimental results show that the gradient-based algorithms are significantly more effective when the number of hyperparameters is large. 

\begin{table}
\footnotesize
% Алгоритм & Тип алгоритм & Время & Плюсы & Минусы 

\begin{tabularx}{\textwidth}{|p{2cm}|p{2cm}|X|X|}
\hline
\bf Algorthm & \bf Type  & \bf Pros & \bf Cons  \\ \hline
Random search & Stochastic & Easy to implement & Ineffective in high dimensions (curse of dimension)\\ \hline
Greedy~\cite{hyper_greed} & Gradient-based & Can used in inner model parameter optimization& Greedy and non-optimal: the algorithm is correct whenever the loss function Hessian is identity. \\ \hline
HOAG~\cite{hyper_hoag} & Gradient-based & Fast convergence & The algorithm requires additional parameter configuration: it is required to solve the linear system, which depends on the Hessian of a loss function. The error in linear system solution influences the final model quality.\\ \hline 
DrMAD~\cite{hyper_mad} & Gradient-based & Considers the trajectory of parameter updates and the optimization algorithm.& Can be instable because of gradient explosion or vanishing. The algorithm is correct when the trajectory of parameter update is linear, which is mostly not true for the complex non-convex models. \\ \hline
\end{tabularx}

\caption{The properties of the analyzed algorithms}
\label{table:algo_descr}

\end{table}


\paragraph{Related works}
In~\cite{random1, random2} propose hyperparameters optimization strategies based on the random search. The current gold-standard methods~\cite{probopt1, probopt2}   propose hyperparameter optimizations based on the probabilistic models construction for the optimal hyperparameter value prediction. These methods can be ineffective whenever the number of hyperparameters is large~\cite{hyper}.

In~\cite{hyper, hyper2,  hyper_mad, hyper_hoag, hyper_greed} the gradient-based hyperparameter optimization methods are proposed. The papers~\cite{hyper_mad,hyper} propose the gradient-based methods, which uses reverse differentiation method. This method is similar to backpropagation. The main idea of this approach is to restore the full history of parameter updates in order to optimize hyperparameters using this history. In general this procedure is computationally expensive. In~\cite{hyper_greed} propose to use only the last update of parameters at each optimization iteration. This method can be considered as a greedy hyperparameter optimization algorithm. In~\cite{hyper} optimize the model parameters using stochastic gradient descent with momentum, which allows to use less memory for storing the parameter update history. In~\cite{hyper_mad} propose to linearize the history trajectory to effectively restore the update history. In~\cite{hyper_hoag} use an approximation of the exact hyperparameter gradient, which can be calculated for when the problem satisfies some regular conditions.

For the hyperparameter optimization various model selection criteria can be used~\cite{MacKay,Bishop}. 
In~\cite{MacKay,Bishop,tokmakova, strijov_dsc} use marginal likelihood or \textit{evidence}. In~\cite{tokmakova, strijov_dsc} consider the problem of model selection and hyperparameter optimization for the linear regression problem. One of the methods of the marginal likelihood approximation is an \textit{evidence lower bound}, which can be obtained using variational inference~\cite{Bishop}. In~\cite{hoffman} propose a stochastic version of the variational inference. In~\cite{varmc} analyze the relation between the gradient-based evidence lower bound estimations and MCMC methods. An alternative model selection criterion is a minimum description length~\cite{mdl} that characterizes statistical complexity of the model. In~\cite{nips}  propose an evidence lower bound estimation method for the neural networks. The authors analyze the relation between the evidence lower bound and minimum description length. 

The other model selection criterion is the cross-validation~\cite{cv_ms, tokmakova}. One of the problem of this criterion is its high computational cost~\cite{expensive,expensive2}. In~\cite{bias,bias2} analyze the variance of cross-validation model perforance estimation. In~\cite{tokmakova} compare hyperparameter values obtained during hyperparameter optimization using various model selection criteria.



\section{Problem statement}
There given a dataset \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m.\end{equation} It contains the object matrix $\mathbf{X}$ and the label vector  $\mathbf{y}$. The label  ${y_i}$ is in finite set, ${y}_i \in \mathbb{Y} = \{1, \dots, Z\}$, in case of $Z$-class classification problem. For the regression problem the label $y_i$ is in the real-value subset ${y_i} \in  \mathbb{R}$.

A model $f$ predicts the variable~$y_i$:
\[
	f:\mathbb{R}^n \to \mathbb{Y}, \quad \mathbf{w} \in \mathbb{R}^u.
\]
It is differentiable with respect to parameters.

Introduce the probabilistic interpretation of the model $f$ for the classification and regression problems.

\paragraph{Regression problem. }
Suppose the dependent variable $y$ is normally distributed:
\begin{equation}
\label{eq:reg}
\mathbf{y} = \mathcal{N}\bigl(\mathbf{f}, \mathbf{I}\bigr),\quad \text{where }\mathbf{f} = \mathbf{f}(\mathbf{w}, \mathbf{X}).
\end{equation}


Define the likelihood function $p(\mathbf{y}|\mathbf{X}, \mathbf{w})$:
\[
	\text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}) =-\frac{m}{2}\text{log}(2\pi)   -\frac{1}{2}\bigl(\mathbf{y} - \mathbf{f}(\mathbf{w}, \mathbf{X}))^\mathsf{T}\mathbf{I}(\mathbf{y} - \mathbf{f}(\mathbf{w}, \mathbf{X})\bigr).
\] 

\paragraph{Classification problem. }
For the 2-class classification problem suppose the dependent variable $y$  is distributed binomially:
\begin{equation}
\label{eq:cl}
	\mathbf{y} = \mathcal{B}\bigl(1-\mathbf{f}, \mathbf{f}\bigr),
\end{equation}
where  $\mathbf{f}$ is the probability that objects from the matrix $\mathbf{X}$  belong to the first class.
For the multiclass classification problem the  dependent variable $y$  is distributed multinomially and $r$-th component ${f}_r$ is the probability that objects from the matrix $\mathbf{X}$  belong to the class $r$. Define the likelihood function
\[
	\text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}) = \sum_{\mathbf{x}, y \in \mathbf{X}, \mathbf{x}} \sum_{r=1}^Z[y=r] \text{log}{f}_r(\mathbf{w}, \mathbf{x}).
\] 

For both classification~\eqref{eq:cl} and regression~\eqref{eq:reg} problems define the prior distribution~$p(\mathbf{w}|\mathbf{A})$:
\begin{equation}
\label{eq:prior}
	\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1}),
\end{equation}
where  $\mathbf{A}^{-1} = \text{diag}[\alpha_1, \dots, \alpha_u]^{-1}$ is the covariance  diagonal matrix. The hypotheses~\eqref{eq:reg},~\eqref{eq:cl} and~\eqref{eq:prior} 	do	not	contradict	each other since the normal distribution is unbounded~\cite{bayes_constr}.  %TODO: change ref

%Model selection methods consider not only the model parameters $\mathbf{w}$, but also information about the posterior distribution $p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A})$. Therefore distinct the model parameters vector $\mathbf{w}$ and vector of parameters for model selection $\boldsymbol{\theta}$. These vectors can be equal in simple case, when the true positerior distribution $p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A})$ estimation is available or when the model predictive performance is evaluated using only one instance of model parameters. 

Since the high dimensionality  of parameter space and non-convexity of loss functions some model selection methods use parameters of multiple models instances. The authors denote by $\mathbf{w}$ a vector of model parameters and denote by $\boldsymbol{\theta} \in \mathbb{R}^s$ a vector of parameters for multiple instances or for a distribution of model parameters. The details of $\boldsymbol{\theta}$ construction are given in the model selection methods description.

Introduce an example of hyperparameter optimization using the coherent Bayesian inference. Optimize model parameters $\boldsymbol{\theta} = \mathbf{w}$ according to the model parameter distribution $p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A})$:
\begin{equation}
\label{eq:bayes1}
\hat{\boldsymbol{\theta}} = \argmax \bigl(-L(\boldsymbol{\theta}, \mathbf{A})\bigr) = p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A}) = \frac{p(\mathbf{y}|\mathbf{X},\mathbf{w})p(\mathbf{w}|\mathbf{A})}{p(\mathbf{y}|\mathbf{X},\mathbf{A})}.
\end{equation}

Calculate the hyperparameter  $\mathbf{A}$ posterior distribution:
\[
p(\mathbf{A}|\mathbf{X}, \mathbf{y}) \propto p(\mathbf{y}|\mathbf{X},\mathbf{A})p(\mathbf{A}),
\]
where $\propto$ means proportionality.

Suppose $p(\mathbf{A})$ is uniform on a large interval. The hyperparameter optimization problem is:
\begin{equation}
\label{eq:bayes2}
	Q(\boldsymbol{\theta}, \mathbf{A}) = p(\mathbf{y}|\mathbf{X},\mathbf{A}) = \int_{\mathbf{w} \in \mathbb{R}^u} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{A}) \to \max_{\text{diag}(\mathbf{A}) = [\alpha_1, \dots, \alpha_u] \in \mathbb{R}^{n}}.
\end{equation}


%\begin{figure}
%  \includegraphics[width=0.8\linewidth]{slide_plots/hyper.png}
%\label{fig:hyper}
%    \caption{Marginal likelihood dependency on hyperparameter $\alpha$ disrtribution. TODO: переделать}
 
   
%    \end{figure}


Formulate the hyperparameter optimization problem. Given functions $L$ and $Q$ that characterize the model selection method. The loss function $L$ optimizes parameters $\boldsymbol{\theta}$. The validation function $Q$ evaluates the model predictive performance.
The problem is to optimize the parameters $\boldsymbol{\theta}$ and the hyperparameters $\mathbf{A}$: 
\begin{equation}
\label{eq:main}
	\hat{\mathbf{A}} = \argmax_{[\alpha_1, \dots, \alpha_u] \in \mathbb{R}^n} Q(\hat{\boldsymbol{\theta}}(\mathbf{A}), \mathbf{A}),
\end{equation}
\begin{equation}
\label{eq:main2}
	\hat{\boldsymbol{\theta}}(\mathbf{A}) =  \argmin_{\boldsymbol{\theta} \in \mathbb{R}^s} L(\boldsymbol{\theta}, \mathbf{A}).
\end{equation}

Introduce the functions $L, Q$ and $\boldsymbol{\theta}$ for various model selection methods.

\paragraph{Basic method.}
Optimize the model parameters $\boldsymbol{\theta}$ and hyperparameters $\mathbf{A}$ using the whole dataset $\mathfrak{D}$ and the same function for both optimization and model evaluation with one model instance, $L = -Q$:
$$ Q(\boldsymbol{\theta}, \mathbf{A}) = -L(\boldsymbol{\theta}, \mathbf{A})  = \text{log}p(\mathbf{y}, \mathbf{w} | \mathbf{X}, \mathbf{A}) = \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w})+\text{log}p(\mathbf{w}|\mathbf{A})$$.

\paragraph{Cross-validation.}
Split the dataset  $\mathfrak{D}$ into $k$ equal parts:
$
\mathfrak{D} = \mathfrak{D}_1 \sqcup \dots \sqcup \mathfrak{D}_k.
$


Run $k$ model optimizations for each subset. The vector $\boldsymbol{\theta}$ is a concatenation of model parameters $\mathbf{w}_k$ for each optimization instance:
\[
\boldsymbol{\theta} = [\mathbf{w}_1, \dots, \mathbf{w}_k].
\]
 
Construct the loss function $L$ as an average negative log posterior probability over the remaining part of each split $\mathfrak{D}_1, \dots, \mathfrak{D}_k$:  
\begin{equation}
\label{eq:cv}
L(\boldsymbol{\theta}, \mathbf{A}) = -\frac{1}{k}\sum_{c=1}^k \bigl(\frac{k}{k-1}\text{log}p(\mathbf{y} \setminus \mathbf{y}_c|\mathbf{X}\setminus \mathbf{X}_c, \mathbf{w}_c) + \text{log}p(\mathbf{w}_c|\mathbf{A})\bigr).
\end{equation}

Construct the validation function $Q$ as an average log likelihood over $k$ splits:
\[
Q(\boldsymbol{\theta}, \mathbf{A}) = \frac{1}{k}\sum_{c=1}^k k\text{log}p(\mathbf{y}_c|\mathbf{X}_c, \mathbf{w}_c).
\]

\paragraph{Evidence Lower Bound.}
As in the basic method description let $L=-Q$. It is an evidence lower bound:
\begin{equation} 
\label{eq:elbo}
\text{log}~p(\mathbf{y}|\mathbf{X},\mathbf{A})  
\geq 
-\text{D}_\text{KL} \bigl(q(\mathbf{w})||p(\mathbf{w}|\mathbf{A})\bigr) + \int_{\mathbf{w}} q(\mathbf{w})\text{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{w},\mathbf{A})} d \mathbf{w}  \approx
\end{equation}
\[
\approx \sum_{i=1}^m \text{log}~p({y}_i|\mathbf{x}_i, \mathbf{w}_i) - D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{A})\bigr) = -L(\boldsymbol{\theta}, \mathbf{A}) = Q(\boldsymbol{\theta}),
\]
where $q$ is an auxiliary distribution called variational distribution. Let $q$ be a normal distribution:
\begin{equation}
\label{eq:diag}
	q \sim \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q),
\end{equation}
where $\mathbf{A}_q = \text{diag}[\alpha^q_1, \dots, \alpha^q_u]^{-1}$ is  a diagonal covariance matrix and $\boldsymbol{\mu}_q$ is a mean vector.
The divergence $D_\text{KL}$ between 2 Gaussian variables is 
\[
	D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{f})\bigr) = \frac{1}{2} \bigl( \text{Tr} [\mathbf{A}\mathbf{A}^{-1}_q] + (\boldsymbol{\mu} - \boldsymbol{\mu}_q)^\mathsf{T}\mathbf{A}(\boldsymbol{\mu} - \boldsymbol{\mu}_q) - u +\text{ln}~|\mathbf{A}^{-1}| - \text{ln}~|\mathbf{A}_q^{-1}| \bigr).
\]
The vector of optimized parameters $\boldsymbol{\theta}$ is a vector of variational distribution $q$ parameters:
\[
\boldsymbol{\theta} = [\text{diag}(\mathbf{A}_q), {\mu}_1,\dots,{\mu}_u].
\]




\section{Gradient-based hyperparameter optimization methods}
In this section we analyze hyperparameter optimization methods based on gradient descent. Suppose the parameters $\boldsymbol{\theta}$ are also optimized using gradient-based methods. 

\textbf{Definition.} A stochastic gradient descent operator $T$ estimates the parameters $\boldsymbol{\theta}^\prime$ using their previous values~$\boldsymbol{\theta}$ using random subset of dataset:
\[
	\boldsymbol{\theta}^\prime = T(\boldsymbol{\theta}, \mathbf{A}, \mathfrak{D}) = \boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A})_{\mathfrak{D} = \hat{\mathfrak{D}}},
\]
where $\hat{\mathfrak{D}}$ is a random subset of $\mathfrak{D}$.

Optimize the parameters  $\boldsymbol{\theta}$ with $\eta$ steps of stochastic gradient descent:
\begin{equation}
\label{eq:gd}
	 \hat{\boldsymbol{\theta}} = T \circ T \circ \dots \circ T(\boldsymbol{\theta}_0, \mathbf{A}) = T^\eta(\boldsymbol{\theta}_0, \mathbf{A}), \quad 	T(\boldsymbol{\theta}, \mathbf{A}) =\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A}), 
\end{equation}
where $\gamma$ is the learning rate, $\boldsymbol{\theta}_0$ is the initial values of the vector $\boldsymbol{\theta}$. 

Redefine the optimization problem according to the definition of operator $T$:
\begin{equation}
\label{eq:optim}
	\hat{\mathbf{A}} = \argmax_{\text{diag}(\mathbf{A}) \in \mathbb{R}^n} Q\bigl( T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr).
\end{equation}
Solve the optimization problem~\eqref{eq:optim} using gradient-based methods. The exact gradient calculation $\nabla_{\mathbf{A}} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{A}))$ is intractable because of the inner optimization operator $T$. The scheme of the hyperparameter optimization.
\begin{enumerate}
\item In range from 1 to  $l$, where $l$ is the number of the hyperparameter optimization iterations:
\item Initialize parameters $\boldsymbol{\theta}$.
\item Solve optimization problem~\eqref{eq:optim} and obtain the new hyperparameter values $\mathbf{A}'$
\item Set $\mathbf{A} = \mathbf{A}'$.
\end{enumerate}

Introduce methods of numerical solution for this optimization problem.

\paragraph{Greedy algorithm.}
Update the hyperparameter $\mathbf{A}$ using gradient descent, which depends only on the last update of parameters $\boldsymbol{\theta}$. Optimize the hyperparameters and parameters in a single optimization procedure. Update the hyperparameter $\mathbf{A}$ at every iteration:
\[
	\mathbf{A}' = \mathbf{A} - \gamma_{\mathbf{A}} \nabla_{\mathbf{A}}  Q \bigl(T(\boldsymbol{\theta}, \mathbf{A}) , \mathbf{A}\bigr) = \mathbf{A} - \gamma_{\mathbf{A}} \nabla_{\mathbf{A}}  Q\bigl(\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A}), \mathbf{A})\bigr),
\]
where $\gamma_{\mathbf{A}}$ is the learning rate for the hyperparameter optimization.

\paragraph{HOAG.}
Obtain approximate hyperparameter gradient values  $\nabla_{\mathbf{A}} Q \bigl(T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr)$ using the approximation of the following  formula of the gradient:
\[
\nabla_{\mathbf{A}} Q \bigl(T^\eta(\boldsymbol{\theta}_0, \mathbf{A})\bigr) = \nabla_{\mathbf{A}} Q(\boldsymbol{\theta}, \mathbf{A}) - (\nabla^2_{\boldsymbol{\theta}, \mathbf{A}} L(\boldsymbol{\theta}, \mathbf{A}))^\mathsf{T}\mathbf{H}(\boldsymbol{\theta})^{-1}\nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{A}),
\]
where $\mathbf{H}$ is the Hessian of the function $L$ with respect to the parameters $\boldsymbol{\theta}$.

The algorithm of hyperparameter gradient $\nabla_{\mathbf{A}} Q$  approximation:
\begin{enumerate}
\item Optimize the parameters $\boldsymbol{\theta} = T^\eta(\boldsymbol{\theta}_0, \mathbf{A})$.
\item Solve linear system for a vector $\boldsymbol{\lambda}$: $\mathbf{H}(\boldsymbol{\theta})\boldsymbol{\lambda} =  \nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{A})$.
\item Compute the approximate hyperparameter gradient: $\hat{\nabla}_{\mathbf{A}}Q = \nabla_{\mathbf{A}}Q(\boldsymbol{\theta}, \mathbf{A}) -\nabla_{\boldsymbol{\theta}, \mathbf{A}} L(\boldsymbol{\theta}, \mathbf{A})^\mathsf{T}\boldsymbol{\lambda}$.
\end{enumerate}

The final update rule is
\begin{equation}
\label{eq:update_hyper}
\mathbf{A}' = \mathbf{A} - \gamma_{\mathbf{A}} \hat{\nabla}_{\mathbf{A}}Q.
\end{equation}
The computational cost of the Hessian $\mathbf{H}(\boldsymbol{\theta})$ evaluation  from step 2 is high.  In this paper we use stochastic gradient descent for solving this linear system. 


\paragraph{DrMad.}
For the hyperparameter gradient evaluation restore the full history of $\eta$ parameters updates starting from the initial value $\boldsymbol{\theta}_0$.  For the simplification of this procedure suppose that the trajectory of parameters $\boldsymbol{\theta}$ update is linear:
\begin{equation}
\label{eq:mad_lin}
\boldsymbol{\theta}^\tau = \boldsymbol{\theta}_0 + \frac{\tau}{\eta} T(\boldsymbol{\theta}).
\end{equation}

The algorithm of approximate hyperparameter gradient calculation:
\begin{enumerate}
\item Optimize  parameters $\boldsymbol{\theta} = T^\eta(\boldsymbol{\theta}_0, \mathbf{A})$.
\item Let $\hat{\nabla} \mathbf{A} = \nabla_\mathbf{A} Q(\boldsymbol{\theta}, \mathbf{A}).$ 
\item Let $d\mathbf{v} = \mathbf{0}.$
\item In range from $\tau = \eta$ to  1:
\item Compute $\boldsymbol{\theta}^\tau$\eqref{eq:mad_lin}.
\item $d\mathbf{v} =  \gamma \hat{\nabla}_{\boldsymbol{\theta}}$.
\item $\hat{\nabla} \mathbf{A} =  \hat{\nabla} \mathbf{A} - d\mathbf{v}\nabla_{\mathbf{A}} \nabla_{\boldsymbol{\theta}} Q$.
\item $\hat{\nabla} \boldsymbol{\theta}  = \hat{\nabla} \boldsymbol{\theta}  - d\mathbf{v}\nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} Q$.
\end{enumerate}
The final update rule is analogous to~\eqref{eq:update_hyper}.
The algorithm is instable when the learning rate $\gamma$ is large~\cite{hyper_mad}. For the stability of the algorithm we discard first 5\% of values  $\boldsymbol{\theta}$ and calculate only each $\tau_\text{step}$ step of parameter updates history:
\begin{equation}
\label{eq:mad_lin2}
\boldsymbol{\theta}^\tau = \boldsymbol{\theta}_{\tau_0} + \frac{\tau}{\eta} T(\boldsymbol{\theta}), \quad \tau \in \{\tau_0, \tau_o + \tau_\text{step}, \dots,\eta - \tau_\text{step}, \eta\},
\end{equation}
where $\tau_0 = \ceil{0.05 \cdot \eta}$.

\section{Experiments}
For the evaluation of the analyzed algorithms we conducted a series of computational experiments. A synthetic dataset, WISDM~\cite{wisdm} and MNIST~\cite{mnist} datasets were used. All datasets were spitted into Train and Test subsets. The  algorithms were evaluated on the Test subsets. For each dataset and each algorithm we ran 5 optimizations. The results were averaged. 
We used the following evaluation criteria.
\begin{enumerate}
\item Quality: the best value of $Q$:  $\hat{Q} = \max_{j \in \{1, \dots, l\}}Q^j$.
\item Convergence: the number of iterations to have a validation value greater than 99\% of the best value $\hat{Q}$:
\[
    \argmin_{j}: \frac{Q^j - Q^0}{\hat{Q} - Q^0} \geq 0.99,
\]
where $Q^0$ is the value of $Q$ before the hyperparameter optimization. 


\item Error function $E$:\[
    E = \text{RMSE}(\mathbf{X}_\text{test}, \mathbf{y}_\text{test}) = \left (\frac{1}{|\mathbf{X}_\text{test}|}\sum_{\mathbf{x} \in\mathbf{X}_\text{test}, y \in \mathbf{y}_\text{test}}  (f(\mathbf{x}, \mathbf{w})-y)\right)^{\frac{1}{2}}
\]
for the regression problem and
\[
    E = \text{Accuracy}(\mathbf{X}_\text{test}, \mathbf{y}_\text{test}) = 1 - \frac{1}{|\mathbf{X}_\text{test}|}\sum_{\mathbf{x} \in\mathbf{X}_\text{test}, y \in \mathbf{y}_\text{test}} [f(\mathbf{x}, \mathbf{w}) \neq y]
\]
for the classification problem.


\item The error function  $E_\sigma$ for the model with noisy dataset:
\[
    \text{RMSE}_\sigma =  \text{RMSE}(\mathbf{X}_\text{test} +  \boldsymbol{\varepsilon}, \mathbf{y}_\text{test}), \quad \text{Accuracy}_\sigma =  \text{Accuracy}(\mathbf{X}_\text{test} +  \boldsymbol{\varepsilon}, \mathbf{y}_\text{test}),  \quad \boldsymbol{\varepsilon} \sim \mathcal{N}\bigl(\mathbf{0}, \sigma\mathbf{I}\bigr).
\]
\end{enumerate}

%Для каждого алгоритма для синтетической выборки было проведено 50 повторений, результаты были усреднены. 
%Для остальных выборок было проведено 5 повторений. 
The random search was used as a baseline. The number of the random search iterations equaled to the number $l$ of gradient-based hyperparameter optimization algorithms iterations. It was set to $l=50$ for the synthetic dataset and WISDM dataset, $l=25$ for the MNIST dataset. The main properties of the algorithms are listed in Table~\ref{table:algo_descr}. For the loss  function $L$ and validation function $Q$ cross-validation~\eqref{eq:cv} with $k=4$ and evidence lower bound~\eqref{eq:elbo} were used. 

\begin{table}
\small
% Алгоритм & Тип алгоритм & Время & Плюсы & Минусы 

\begin{tabularx}{\textwidth}{|X|X|X|X|}
\hline
\bf Algorithm & \bf Type & \bf Optimization iteration complexity & \bf Correctness suppositions  \\ 
\hline
Random search & stochastic & $O(\eta s |\hat{\mathfrak{D}}|)$& -  \\ \hline
Greedy~\cite{hyper_greed} & gradient-based & $O(\eta (s+h) |\hat{\mathfrak{D}}|)$ & $\mathbf{H}(\boldsymbol{\theta}) = \mathbf{I}$  \\ \hline
HOAG~\cite{hyper_hoag} & gradient-based & $O(\eta s |\hat{\mathfrak{D}}| + h^2 |\hat{\mathfrak{D}}| + \text{o}),$ where $o$ is a complexity of linear equation solution& first derivatives of $Q$  and second derivatives of $L$ are Lipshitz functions;  $\text{det}\mathbf{H} \neq \mathbf{0}$;  \\ \hline
DrMAD~\cite{hyper_mad} & gradient-based &$O(\eta s |\hat{\mathfrak{D}}|)$ & Parameter trajectory$\boldsymbol{\theta}$ = $\boldsymbol{\theta}_0, \dots \boldsymbol{\theta}_\eta$  is linear \\ \hline
\end{tabularx}

\caption{Complexity and correctness of the analyzed algorithms}
\label{table:algo_descr}

\end{table}


\begin{figure}[tbh!]
    \centering
    \includegraphics[width=0.5\linewidth]{plots/traj.eps}

    \caption{An example of parameter update trajectories. The color displays the value of the validation function $Q$. Greedy algorithm optimizes hyperparameter during the parameter optimization, therefore it has the light blue trajectory between the optimized green trajectory of HOAG algorithm and the dark blue trajectory of parameters without hyperparameter optimization. DrMAD uses a linearized dashed parameter trajectory during hyperparameter optimization procedure.  }
    \end{figure}


The hyperparameters were initialized with uniform distribution:\[
    \text{diag}(\mathbf{A}) \sim \mathcal{U}(a,b)^n,
\]
where $a = -2, b = 10$ for the synthetic dataset and $a = -4, b = 10$  for the WISDM and MNIST datasets.

We calibrated the hyperparameter learning rate $\gamma_{\mathbf{A}}$  for each algoritm using grid search:~$\{r \cdot 10^{s}, s \leq 1, r \in \{1,25,50,75\}\}$. The largest value of $\gamma_{\mathbf{A}}$ was chosen if the final hyperparameter value $\mathbf{A}$ satisfied the condition:
\[
    a_\text{min} \leq  \min(\mathbf{A}), \quad \max(\mathbf{A}) \leq b_\text{max},
\] 
where $a_\text{min} = -2.5, b_\text{max}=10.5$ for the synthetic dataset and $a_\text{min} = -5, b_\text{max}=11$ for the WISDM and MNIST datasets. We calibrated $\gamma_\mathbf{A}$ with a small amount of iterations: $l=50$ for the syntetic dataset, $l=10$ for the WISDM and $l=5$ for the MNIST dataset. Whenever each algorithm showed instability in further experiments (gradient explosion or overflow) the value of $\gamma_{\mathbf{A}}$ was lowered. The parameter $\tau_\text{step}$ set to $1$ for syntetic dataset and WISDM and  $\tau_\text{step}=10$ for the MNIST dataset.



\paragraph{Synthetic dataset.}
The synthetic dataset was generated according to the rule:
\[
	\mathbf{y} = \mathbf{x} + \boldsymbol{\varepsilon},\quad \mathbf{x}  \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
\]
where $\quad m = 40, \quad n = 1.$

The regression model $\mathbf{f}$ uses the following features: $\{\mathbf{x}^0, \dots, \mathbf{x}^9, \textbf{sin}(\mathbf{X}), \textbf{cos}(\mathbf{X})\}$. The unregularized regression model with this such ratio of  object number and feature number tends to significantly overfit. The goal of this experiment was to analyze if the hyperparameter optimization could protect such models from overfit.

Fig.~\ref{fig:poly} plots the resulting polynoms. 
The evidence lower bound criterion~\eqref{eq:elbo} gave non-overfitted polynoms, which parameters are close to the linear models.

\paragraph{WISDM.}
The WISDM dataset contains a set of records from accelerometer. Each record has three coordinates that correspond to the accelerometer axes. As a set of features we used 199 sequential records. We used $l_2$ norm of the 200th records as a label and required to predict. 

A neural network with 10 neurons on the hidden layer was used:
\[
    \mathbf{f} = \mathbf{W}_2 \cdot \textbf{RELU}(\mathbf{W}_1\mathbf{X} + \mathbf{b}_1) +\mathbf{b}_2,
\]
where $\mathbf{W}_1, \mathbf{b}_1$ --- hidden layer parameters,
$\mathbf{W}_2, \mathbf{b}_2$ --- output layer parameters,
\[
    \textbf{RELU}(\mathbf{x}) = \max(\mathbf{0}, \mathbf{x}).
\]

Fig.~\ref{fig:wisdm} shows the plots with RMSE and best validation loss value $\hat{Q}$ for the WISDM dataset.
The DrMad and HOAG algorithms showed significantly lower results than greed algorithm. The random search showed good results in case of cross-validation, when the number of hyperparameters is small. When the number of hyperparameters is large the greedy and HOAG algorithm showed good results. HOAG requires more iteration number $l$ than the greedy algorithm for the convergence.


\paragraph{MNIST.}
The MNIST dataset contains a set of images of handwritten digits. 
We used a neural network with 300 neurons on the hidden layer and softmax output.


Fig.~\ref{fig:mnist} plots the error $E$ and quality $\hat{Q}$.
For the evidence lower bound criterion, the models with highest $\hat{Q}$ value have also highest error $E$. Nevertheless, 
these models showed the best results for the stability, when the noise in dataset is high. Fig.~\ref{fig:noise} plots  the error $E_\sigma$. Models with highest evidence lower bound have the lowest error when the Test dataset is noisy. We interpret this as an ability to select the model with highest generalization ability.

As we can see from the experiments, the gradient-based methods give better results than the random search when the number of hyperparameters is large. The best results for all the experiments were obtained by greedy algorithm. The DrMad algorithm showed weaker results than greedy and HOAG algortihms because of its instability: the algorithm often had gradient explosions, therefore the learning rate $\gamma_{\mathbf{A}}$ for it was set to low values during calibration. 



\begin{table}
\small
\begin{tabularx}{\textwidth}{ |X|X|X|X|X|X|X|X|X|}

\hline
\textbf{Algorithm} & $L, Q$  & $Q(\boldsymbol{\theta}, \mathbf{A})$ & Convergence & E & $E_{0.25}$ & $E_{0.5}$\\ 
\hline
\multicolumn{7}{|c|}{\textit{Synthetic}}  \\
\hline
Random search & ~\eqref{eq:cv} & \bf -171.6  &\bf 26.2 $\pm$ 20.0  & \bf 1.367 &\bf 1.410 &\bf 1.555 \\
\hline
Greedy & ~\eqref{eq:cv} & -172.5 & 30.0 $\pm$ 24.5 & 1.421 & 1.439 &  1.536\\
\hline
DrMAD & ~\eqref{eq:cv} & -174.1 & 40.2 $\pm$ 16.1 &  1.403 & 1.424 & \bf 1.512 \\
\hline
HOAG & ~\eqref{eq:cv} &-174.7 & 29.4 $\pm$ 24.0 &   \bf 1.432  & 1.463 & 1.553\\
\hline
Random Search & ~\eqref{eq:elbo} & -63.5  & 32.4 $\pm$ 18.7  & 1.368 & 1.426 & 1.546  \\
\hline
Greedy & ~\eqref{eq:elbo} & -25.5 & \bf 1.2 $\pm$ 0.4 & 1.161 & 1.174 & 1.193\\
\hline
DrMAD & ~\eqref{eq:elbo} & \bf -25.1 &  10.6 $\pm$ 0.8 &  1.157 & 1.163 &  1.184\\
\hline
HOAG & ~\eqref{eq:elbo} &-25.8 & 10.8 $\pm$ 1.5&   \bf 1.141  & \bf 1.149 & \bf 1.177\\
\hline


\multicolumn{7}{|c|}{\textit{WISDM}}  \\
\hline
Random search & ~\eqref{eq:cv} & \bf -1086661.1  & 22.0 $\pm$ 19.3  & \bf 0.660 & \bf 0.670 & \bf 0.690  \\
\hline
Greedy & ~\eqref{eq:cv} & -1086707.1 & \bf 15.4 $\pm$ 17.2 & 0.707 &  0.723  &  0.769\\
\hline
DrMAD & ~\eqref{eq:cv} & -1086708.2 & 29.2 $\pm$ 8.0 &  0.694 &  0.708 & 0.742 \\
\hline
HOAG & ~\eqref{eq:cv} & -1086733.5 & 28.2 $\pm$ 7.13&   0.701 & 0.724 & 0.753 \\
\hline
Random search & ~\eqref{eq:elbo} & -35420.4 &   14.4 $\pm$ 7.8  &   0.732 &   0.755 & 0.785 \\
\hline
Greedy & ~\eqref{eq:elbo} & \bf -3552.9 &\bf 1.0 $\pm$ 0.0  &   \bf 0.702 & \bf 0.730  &  \bf 0.767\\
\hline
DrMAD & ~\eqref{eq:elbo} & -26091.4 &   50.0 $\pm$ 0.0  & 0.729 &  0.753 & 0.816 \\
\hline
HOAG & ~\eqref{eq:elbo} &  -16566.6 & 49.0 $\pm$ 0.0  &  0.733 &  0.755 &  0.801 \\
\hline



\multicolumn{7}{|c|}{\textit{MNIST}}  \\
\hline
Random search & ~\eqref{eq:cv} & -3236.4  & 7.8 $\pm$ 1.9  &   0.981 & \bf 0.966 & \bf 0.866 \\
\hline
Greedy & ~\eqref{eq:cv} & \bf -3416.7 & 10.8 $\pm$ 10.4 & 0.979 & 0.962 & 0.860\\
\hline
DrMAD & ~\eqref{eq:cv} & -3469.0 & 17.0 $\pm$ 5.6 & \bf  0.982 & 0.962 & 0.831\\
\hline
HOAG & ~\eqref{eq:cv} & -3748.6 & \bf 8.6 $\pm$ 7.3&   0.980 &  0.961  & 0.853 \\
\hline
Random search & ~\eqref{eq:elbo} & -1304556.4 &  14.2 $\pm$ 5.7 &  \bf 0.982 & 0.943 & 0.814 \\
\hline
Greedy & ~\eqref{eq:elbo} & \bf -11136.2 & \bf 1.0 $\pm$ 0.0  &  0.977 & \bf 0.952 & \bf 0.884\\
\hline
DrMAD & ~\eqref{eq:elbo} & -1305432.9 & 24.6 $\pm$ 0.5  & \bf 0.982 & 0.941 & 0.813 \\
\hline
HOAG & ~\eqref{eq:elbo} &  -280061.6 & 24.0 $\pm$ 0.0  & 0.981 & 0.943 & 0.819\\
\hline


\hline
\end{tabularx}
\caption{Experiment results}
\label{table:table}
\end{table}

    \begin{figure}

    \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\linewidth]{plots/poly_cv.pdf}

    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\linewidth]{plots/poly_var.pdf}

    \end{subfigure}

 \caption{Resulting models for the synthetic dataset: а --- cross-validation, b --- evidence lower bound}
  \label{fig:poly}
   
    \end{figure}




    \begin{figure}

    \includegraphics[width=\linewidth]{plots/wisdm.eps}
\caption{WISDM,  best validation value $\hat{Q}$ and RMSE  for cross-validation (left) and evidence lower bound (right)}    
\label{fig:wisdm}
    
    \end{figure}


    \begin{figure}

    \includegraphics[width=\linewidth]{plots/mnist.eps}

    \caption{MNIST, best validation value  $\hat{Q}$ and Accuracy  for cross-validation (left) and evidence lower bound (right)}
    \label{fig:mnist}
    \end{figure}

    \begin{figure}

    \includegraphics[width=\linewidth]{plots/noise.pdf}

    \caption{MNIST, Model accuracy with noise in the Test dataset. The hyperparameters were optimized with evidence lower bound criterion.}
    \label{fig:noise}
    \end{figure}





\section{Discussion}
The experiments showed that each algorithm can perform effectively and therefore the appropriate hyperparameter optimization method should rely on the amount of hyperparameters and the specific of the problem. 

When dealing with small amount of hyperparameters the random search showed the best results since the search procedure can be employed more effectively than gradient-based optimization in low-dimensional hyperparameters space. For the high-dimensional hyperparameter space both the  HOAG and greed algorithms showed good performance. 
The HOAG algorithm is more preferable if the model optimization problem is expensive. On the other hand we can schedule greed hyperparameter optimization to make it less expensive as in~\cite{hyper_greed}. 

The DrMad algorithm showed rather poor results on the MNIST and WISDM datasets. Perhaps it is because of high learning rate $\gamma$ we used in experiments. The large value of the learning rate can make the DrMad algorithm instable. Two improvements can be proposed. We can use more stable optimization like Adam or AdaGrad for both parameter and hyperparameter optimization. The second improvement was proposed to develop in~\cite{hyper_mad}: we can use more complicated parameter trajectory approximation to make it more similar to the original parameter trajectory. Opposing to the HOAG and greedy algorithms, the DrMad optimization has prerequisites for optimization not only hyperparameters but also the metaparameters, i.e. the parameters  of the optimization procedure. The opportunity of such optimization using reversed differentiation was shown in~\cite{hyper_mad}. 

The other interesting aspect of our experiments is the relation between the model error (RMSE or Accuracy) and the value of validation loss $Q$. The models obtained by the evidence lower bound showed higher errors than the models obtained using cross-validation on the MNIST and WISDM datasets. Nevertheless these models also showed greater stability when the noise was added to the Test datasets.  The evidence lower bound showed significantly better results on the synthetic dataset, when the amount of object in the Train dataset is small. Therefore we can conclude that the evidence lower bound usage is preferable when the model tend to overfit or when the cross-validation usage is too computationally expensive.   In~\cite{nips} note that the evidence lower bound optimization required more iterations for the convergence. In our experiments we used the same number of iterations both for the cross-validation and evidence lower bound. The more accurate iteration number calibration can improve the final quality of these models. 

\section{Conclusion and future work}
The paper analyzed the gradient-based hyperparameter optimization algorithms. We adapted the analyzed algorithms for general validation functions and evaluated their performance on the  MNIST and WISDM datasets. Two model selection criteria were compared: the cross-validation and evidence lower bound. 

The experiments showed that the gradient-based algorithms are effective when the number of hyperparameters is large. The results showed that models obtained using evidence lower bound have higher error rater than models obtained using cross-validation, but they are also more stable when the test dataset contain a lot of noise. 

The authors  implemenetd the algorithms as a toolbox available at~\cite{pyfos}. The toolbox is developed in Python using Theano~\cite{theano} and Numpy~\cite{numpy} libraries. 

In the future we are planning to develop the analyzed algorithm and to extend gradient-based algorithms to optimize not only hyperparameters, but also the parameters of the model optimization. The other object of our research will be the difference between the cross-validation and evidence lower bound and the theoretical aspects of their properties for the models with large amount of parameters.

\addcontentsline{toc}{section}{References}
\bibliographystyle{utf8gost71u}
%\bibliographystyle{unsrt}
\bibliography{lit}

\end{document}
